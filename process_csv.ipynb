{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-11T08:44:38.361074Z",
     "start_time": "2026-01-11T08:44:38.353653Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def cluster_cohesion(embeddings, labels):\n",
    "    scores = []\n",
    "    for c in np.unique(labels):\n",
    "        idx = labels == c\n",
    "        if idx.sum() < 2:\n",
    "            continue\n",
    "        sims = cosine_similarity(embeddings[idx])\n",
    "        scores.append(sims.mean())\n",
    "    return float(np.mean(scores))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T08:44:40.373942Z",
     "start_time": "2026-01-11T08:44:40.368339Z"
    }
   },
   "id": "4da65c4856278690"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8674b11a076872cc",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-12T06:33:34.081474Z",
     "start_time": "2026-01-12T06:33:34.040171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning and Segmenting Lyrics...\n",
      "âœ… Processing complete. Total segments: 12767\n"
     ]
    }
   ],
   "source": [
    "# data = pd.read_csv(\"output/songs_with_lyrics_cleaned.csv\")\n",
    "# separated_lyrics = data['lyrics'].dropna()\n",
    "\n",
    "musical_data = pd.read_csv(\"dataset/musical.csv\")\n",
    "separated_lyrics = musical_data['Lyrics'].dropna()\n",
    "# separated_lyrics = pd.concat([separated_lyrics, musical_data['Lyrics'].dropna()])\n",
    "\n",
    "corpus = []\n",
    "\n",
    "print(\"ðŸ§¹ Cleaning and Segmenting Lyrics...\")\n",
    "\n",
    "for lyric_text in separated_lyrics:\n",
    "    if isinstance(lyric_text, str):\n",
    "        # 1. Clean Tags\n",
    "        text_no_tags = re.sub(r'\\[.*?\\]', ' ', lyric_text, flags=re.DOTALL)\n",
    "        \n",
    "        # 2. Split into lines\n",
    "        raw_segments = text_no_tags.split('\\n')\n",
    "        \n",
    "        # 3. Clean and Filter\n",
    "        clean_lines = [line.strip() for line in raw_segments if len(line.strip()) > 0]\n",
    "        \n",
    "        # 4. THE MAGIC TRICK: \"Sliding Window\" of 2 lines\n",
    "        # We combine Line 1+2, then Line 2+3, then Line 3+4...\n",
    "        for i in range(len(clean_lines) - 1):\n",
    "            # Combine current line and next line\n",
    "            combined_segment = f\"{clean_lines[i]} {clean_lines[i+1]}\"\n",
    "\n",
    "            # Only keep if it's long enough (e.g. > 40 chars)\n",
    "            if len(combined_segment) > 40:\n",
    "                corpus.append(combined_segment)\n",
    "\n",
    "print(f\"âœ… Processing complete. Total segments: {len(corpus)}\")\n",
    "# Now 'corpus' is ready for SBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Filter/replace words"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30734f98f95af4c9"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4387bb0afe28e96a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-12T05:47:59.163903Z",
     "start_time": "2026-01-12T05:47:59.034247Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/Bay_Techatham/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/Bay_Techatham/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Bay_Techatham/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download resources (run once)\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def get_top_keywords_filtered(texts, n=10):\n",
    "    # 1. SETUP LISTS\n",
    "    custom_stops = list(nltk.corpus.stopwords.words('english'))\n",
    "    pop_slang = [\n",
    "        \"lil\", \"gon\", \"bout\", \"em\", \"ayy\", \"uh\", \"huh\", \"ha\", \"vi\", \"doo\", \"wee\",\n",
    "        \"yeah\", \"oh\", \"baby\", \"know\", \"like\", \"got\", \"just\", \"don\", \"ve\", \"ll\", \n",
    "        \"want\", \"need\", \"love\", \"time\", \"way\", \"make\", \"say\", \"come\", \"go\", \"right\",\n",
    "        \"look\", \"good\", \"feel\", \"really\", \"cause\", \"wanna\", \"gonna\", \"gotta\", \"ain\",\n",
    "        \"girl\", \"boy\", \"man\", \"woman\", \"hey\", \"ooh\", \"whoa\", \"shit\", \"fuck\", \"bitch\",\n",
    "        \"nigga\", \"niggas\", \"damn\", \"ass\", \"tell\", \"think\", \"never\", \"back\", \"let\",\n",
    "        \"swag\", \"yuh\", \"hum\", \"who\", \"what\", \"where\", \"why\", \"top\", \"call\", \"put\",\n",
    "        \"gang\", \"thug\", \"bro\", \"pussy\", \"tryna\", \"chick\", \"girls\", \"slatt\", \"mmh\"\n",
    "    ]\n",
    "    \n",
    "    abstract_concepts = [\n",
    "        \"life\", \"day\", \"night\", \"heart\", \"mind\", \"world\", \"everything\", \"nothing\", \n",
    "        \"things\", \"nothin\", \"songs\", \"song\", \"name\", \"eyes\", \"face\", \"voice\", \n",
    "        \"head\", \"hand\", \"hands\", \"god\", \"soul\", \"mind\", \"pain\", \"hope\", \"wish\", \n",
    "        \"fame\", \"lie\", \"lies\", \"truth\", \"word\", \"words\", \"end\", \"reason\", \"part\",\n",
    "        \"told\", \"saw\", \"knew\", \"met\", \"said\", \"made\", \"found\", \"came\", \"went\",\n",
    "        \"die\", \"born\", \"live\", \"dead\", \"death\", \"control\", \"move\", \"wait\", \"hold\",\n",
    "        \"stop\", \"start\", \"change\", \"keep\", \"leave\", \"stay\", \"believe\", \"remember\"\n",
    "    ]\n",
    "    \n",
    "    NAME_REPLACEMENTS = {\n",
    "        \"regina\": \"queen\", \"veronica\": \"girl\", \"heather\": \"student\",\n",
    "        \"hamilton\": \"soldier\", \"burr\": \"man\", \"jefferson\": \"politician\",\n",
    "        \"elphaba\": \"witch\", \"glinda\": \"friend\", \"evan\": \"boy\",\n",
    "        \"connor\": \"friend\", \"usnavi\": \"guy\", \"vanessa\": \"girl\"\n",
    "    }\n",
    "\n",
    "    # 2. APPLY NAME REPLACEMENTS (The Fix)\n",
    "    processed_texts = []\n",
    "    for t in texts:\n",
    "        t_lower = t.lower() # Lowercase first\n",
    "        for name, replacement in NAME_REPLACEMENTS.items():\n",
    "            t_lower = t_lower.replace(name, replacement)\n",
    "        processed_texts.append(t_lower)\n",
    "\n",
    "    # 3. VECTORIZE (Use the processed list!)\n",
    "    all_stops = custom_stops + pop_slang + abstract_concepts\n",
    "    \n",
    "    try:\n",
    "        vec = CountVectorizer(stop_words=all_stops).fit(processed_texts)\n",
    "        bag_of_words = vec.transform(processed_texts)\n",
    "        sum_words = bag_of_words.sum(axis=0) \n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "        \n",
    "        # 4. FINAL CLEANUP (Noun Check)\n",
    "        cleaned_list = []\n",
    "        for word, freq in sorted(words_freq, key=lambda x: x[1], reverse=True):\n",
    "            if len(word) <= 2: continue \n",
    "            \n",
    "            # Check Noun (NN, NNS)\n",
    "            # We wrap it in list [] because pos_tag expects a list of tokens\n",
    "            pos_tag = nltk.pos_tag([word])[0][1]\n",
    "            if pos_tag.startswith('NN'):\n",
    "                cleaned_list.append((word, freq))\n",
    "                \n",
    "        return cleaned_list[:n]\n",
    "        \n",
    "    except ValueError:\n",
    "        # Handles empty clusters\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98d4fdd72a410d18",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T17:21:33.140851Z",
     "start_time": "2026-01-11T17:15:20.097697Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-12 02:15:20,096] A new study created in memory with name: no-name-39618001-509f-482e-8413-a21059baa870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Starting Optuna Optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-12 02:15:37,715] Trial 0 finished with value: 0.2716422975063324 and parameters: {'n_clusters': 48}. Best is trial 0 with value: 0.2716422975063324.\n",
      "[I 2026-01-12 02:15:52,311] Trial 1 finished with value: 0.2559882402420044 and parameters: {'n_clusters': 41}. Best is trial 0 with value: 0.2716422975063324.\n",
      "[I 2026-01-12 02:16:12,704] Trial 2 finished with value: 0.27651482820510864 and parameters: {'n_clusters': 59}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:16:31,916] Trial 3 finished with value: 0.27528074383735657 and parameters: {'n_clusters': 60}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:16:51,851] Trial 4 finished with value: 0.27528074383735657 and parameters: {'n_clusters': 60}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:17:09,412] Trial 5 finished with value: 0.266170859336853 and parameters: {'n_clusters': 50}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:17:26,276] Trial 6 finished with value: 0.26966679096221924 and parameters: {'n_clusters': 53}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:17:40,633] Trial 7 finished with value: 0.258674681186676 and parameters: {'n_clusters': 40}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:17:58,040] Trial 8 finished with value: 0.2759794294834137 and parameters: {'n_clusters': 58}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:18:14,909] Trial 9 finished with value: 0.26997604966163635 and parameters: {'n_clusters': 51}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:18:33,948] Trial 10 finished with value: 0.2706632912158966 and parameters: {'n_clusters': 55}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:18:54,381] Trial 11 finished with value: 0.2712375521659851 and parameters: {'n_clusters': 56}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:19:18,189] Trial 12 finished with value: 0.27605685591697693 and parameters: {'n_clusters': 57}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:19:34,513] Trial 13 finished with value: 0.25920554995536804 and parameters: {'n_clusters': 45}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:19:53,193] Trial 14 finished with value: 0.2712375521659851 and parameters: {'n_clusters': 56}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:20:11,924] Trial 15 finished with value: 0.2759794294834137 and parameters: {'n_clusters': 58}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:20:36,230] Trial 16 finished with value: 0.2678925395011902 and parameters: {'n_clusters': 54}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:20:55,547] Trial 17 finished with value: 0.2759794294834137 and parameters: {'n_clusters': 58}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:21:14,681] Trial 18 finished with value: 0.26942986249923706 and parameters: {'n_clusters': 52}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:21:33,119] Trial 19 finished with value: 0.2622126638889313 and parameters: {'n_clusters': 47}. Best is trial 2 with value: 0.27651482820510864.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "ðŸ† Best K found: 59\n",
      "ðŸ“ˆ Best Silhouette Score: 0.2765\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 0. ENSURE EMBEDDINGS EXIST\n",
    "if 'embeddings' not in globals():\n",
    "    print(\"â³ Generating Embeddings (this happens once)...\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(corpus)\n",
    "\n",
    "\n",
    "# 1. DEFINE THE OBJECTIVE FUNCTION\n",
    "# Optuna will run this function many times with different 'k' values\n",
    "def objective(trial):\n",
    "    k = trial.suggest_int('n_clusters', 40, 60)\n",
    "\n",
    "    model = KMeans(\n",
    "        n_clusters=k,\n",
    "        random_state=42,\n",
    "        n_init=10\n",
    "    )\n",
    "    labels = model.fit_predict(embeddings)\n",
    "\n",
    "    cohesion = cluster_cohesion(embeddings, labels)\n",
    "\n",
    "    return cohesion  # HIGHER = better semantic cohesion\n",
    "\n",
    "# 2. RUN THE OPTIMIZATION\n",
    "print(\"ðŸ¤– Starting Optuna Optimization...\")\n",
    "study = optuna.create_study(direction='maximize')  # We want MAX separation\n",
    "study.optimize(objective, n_trials=20)  # Try 20 different times\n",
    "\n",
    "# 3. PRINT RESULTS\n",
    "print(\"-\" * 40)\n",
    "print(f\"ðŸ† Best K found: {study.best_params['n_clusters']}\")\n",
    "print(f\"ðŸ“ˆ Best Silhouette Score: {study.best_value:.4f}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 4. OPTIONAL: VISUALIZE\n",
    "optuna.visualization.plot_optimization_history(study).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e39b6d9ca8388e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-12T06:34:13.298605Z",
     "start_time": "2026-01-12T06:33:47.169698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§  Analyzing 12767 lyric segments with SBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 3d3d4059-536c-4490-b8b5-1534b093369d)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "   YOUR VISUAL SHOPPING LIST\n",
      "========================================\n",
      "\n",
      "ðŸ“ CLUSTER 1 (Sample: 1294)\n",
      "   Keywords: ['woah', 'shine', 'shake', 'hear', 'sound', 'dat', 'shot', 'dang', 'fight', 'wolf']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 2 (Sample: 1503)\n",
      "   Keywords: ['mine', 'belong', 'home', 'sorry', 'friend', 'beautiful', 'son', 'show', 'shot', 'years']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 3 (Sample: 1355)\n",
      "   Keywords: ['fight', 'men', 'war', 'captain', 'people', 'deserves', 'lives', 'mercy', 'win', 'home']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 4 (Sample: 1678)\n",
      "   Keywords: ['try', 'game', 'thing', 'fight', 'til', 'something', 'line', 'break', 'monster', 'work']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 5 (Sample: 634)\n",
      "   Keywords: ['soldier', 'president', 'alexander', 'politician', 'washington', 'sir', 'york', 'thomas', 'home', 'congress']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 6 (Sample: 1312)\n",
      "   Keywords: ['room', 'light', 'sky', 'dream', 'happens', 'sun', 'storm', 'home', 'dark', 'place']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 7 (Sample: 1544)\n",
      "   Keywords: ['home', 'house', 'tonight', 'party', 'place', 'round', 'city', 'york', 'work', 'piragua']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 8 (Sample: 1218)\n",
      "   Keywords: ['people', 'friends', 'everyone', 'someone', 'friend', 'school', 'thinks', 'guy', 'guess', 'somebody']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 9 (Sample: 1173)\n",
      "   Keywords: ['queen', 'student', 'beautiful', 'wife', 'eliza', 'sister', 'mom', 'mother', 'friend', 'witch']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 10 (Sample: 1056)\n",
      "   Keywords: ['guy', 'set', 'fire', 'son', 'friend', 'house', 'michael', 'dad', 'father', 'daddy']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nðŸ§  Analyzing {len(corpus)} lyric segments with SBERT...\")\n",
    "\n",
    "NUM_CLUSTERS = 10\n",
    "\n",
    "# A. Embed\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(corpus)\n",
    "\n",
    "# B. Cluster\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42)\n",
    "kmeans.fit(embeddings)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# ==========================================\n",
    "# 4. RESULTS (THE \"SHOPPING LIST\")\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"   YOUR VISUAL SHOPPING LIST\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Helper to find words from the clusters\n",
    "def get_top_keywords(texts, n=8):\n",
    "    vec = CountVectorizer(stop_words='english').fit(texts)\n",
    "    bag_of_words = vec.transform(texts)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    return sorted(words_freq, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "df = pd.DataFrame({'text': corpus, 'cluster': cluster_labels})\n",
    "\n",
    "for i in range(NUM_CLUSTERS):\n",
    "    cluster_text = df[df['cluster'] == i]['text'].tolist()\n",
    "    \n",
    "    # Use the new function\n",
    "    keywords = get_top_keywords_filtered(cluster_text)\n",
    "    \n",
    "    print(f\"\\nðŸ“ CLUSTER {i+1} (Sample: {len(cluster_text)})\")\n",
    "    # Print just the words, not the frequency counts\n",
    "    print(f\"   Keywords: {[k[0] for k in keywords]}\")\n",
    "    print(f\"   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4f5bfedd4f5e412",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T09:01:55.441029Z",
     "start_time": "2026-01-11T09:01:55.423297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved as 'kmeans_presets.pkl'\n",
      "âœ… Labels saved as 'preset_labels.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# 1. SAVE THE MODEL\n",
    "# (Make sure 'kmeans' is your trained model variable)\n",
    "# Using joblib is often safer for sklearn models than pickle, but pickle works too.\n",
    "with open('kmeans_presets.pkl', 'wb') as f:\n",
    "    pickle.dump(kmeans, f)\n",
    "print(\"âœ… Model saved as 'kmeans_presets.pkl'\")\n",
    "\n",
    "# 2. SAVE YOUR ASSET MAP\n",
    "# Mapped to match your SBERT Keywords exactly:\n",
    "\n",
    "preset_map = {\n",
    "    0: \"urban_environment\",  # Cluster 1: Money, Piragua, Bag (In The Heights/City)\n",
    "    1: \"urban_environment\",  # Cluster 2: Dance, Shake, Shot (Party/Club Vibe)\n",
    "    2: \"urban_environment\",  # Cluster 3: Heather, Regina, Queen (High School/Social)\n",
    "    3: \"room_environment\",   # Cluster 4: Home, Door, Walk (Domestic/Inside)\n",
    "    4: \"sky_environment\",    # Cluster 5: Sun, Light, Sky, Stars (Lanterns/Magic)\n",
    "    5: \"dark_environment\",   # Cluster 6: Hate, Fight, Care (Conflict/Spooky)\n",
    "    6: \"ocean_environment\",  # Cluster 7: Captain, War, Men, Stand (Epic/Sea)\n",
    "    7: \"fire_environment\",   # Cluster 8: Hamilton, Fire, Burn (Revolution/Rage)\n",
    "    8: \"snow_environment\",   # Cluster 9: Christmas, Merry, Fall (Winter)\n",
    "    9: \"room_environment\"    # Cluster 10: Room, Dream, Sleep (Intimate/Thoughts)\n",
    "}\n",
    "\n",
    "with open('preset_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(preset_map, f)\n",
    "print(\"âœ… Labels saved as 'preset_labels.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Current Corpus Length: 12767\n",
      "ðŸ§  Saved Model Labels:    12767\n",
      "\n",
      "========================================\n",
      "   YOUR VISUAL SHOPPING LIST\n",
      "========================================\n",
      "\n",
      "ðŸ“ CLUSTER 0 (Sample: 1294)\n",
      "   ðŸ·ï¸ Saved Label: ('urban_environment', 1.0)\n",
      "   Keywords: ['woah', 'shine', 'shake', 'hear', 'sound', 'dat', 'shot', 'dang', 'fight', 'wolf', 'home', 'work', 'sigue', 'knock', 'beautiful', 'light', 'sung', 'rise', 'alabanza', 'senora', 'throwin', 'goodbye', 'nahmen', 'heard', 'turn', 'strike', 'piragua', 'lai', 'dance', 'beep', 'voices', 'room', 'guy', 'eleka', 'count', 'watch', 'drownin', 'break', 'til', 'thank', 'write', 'pare', 'bandera', 'tum', 'boo', 'teach', 'boom', 'sept', 'huit', 'neuf', 'daylight', 'respira', 'calor', 'yes', 'blue', 'something', 'tune', 'happens', 'bop', 'meet', 'cinq', 'quÃ©', 'ride', 'heia', 'squi', 'deux', 'trois', 'quatre', 'diggety', 'westerberg', 'odysseus', 'bite', 'mis', 'everyone', 'people', 'melody', 'play', 'mean', 'kind', 'mine', 'hello', 'balls', 'whine', 'esa', 'powerless', 'try', 'music', 'king', 'beat', 'forgotten', 'helpless', 'roar', 'tonight', 'screams', 'doÃ±a', 'claudia', 'requiem', 'please', 'round', 'burn']\n",
      "\n",
      "ðŸ“ CLUSTER 1 (Sample: 1503)\n",
      "   ðŸ·ï¸ Saved Label: ('urban_environment', 0.8)\n",
      "   Keywords: ['mine', 'belong', 'home', 'sorry', 'friend', 'beautiful', 'son', 'show', 'shot', 'years', 'left', 'fight', 'try', 'throwin', 'side', 'thank', 'today', 'tonight', 'wonderful', 'matter', 'meant', 'thing', 'til', 'dear', 'father', 'shine', 'choose', 'country', 'fine', 'someone', 'twenty', 'yes', 'lives', 'alright', 'moment', 'embrace', 'light', 'door', 'woah', 'fall', 'goodbye', 'promise', 'arms', 'deserve', 'please', 'something', 'wife', 'thought', 'bring', 'knock', 'friends', 'blue', 'trade', 'perfect', 'help', 'okay', 'people', 'miss', 'hear', 'holy', 'balls', 'greet', 'hurt', 'break', 'mother', 'means', 'gay', 'place', 'needs', 'cry', 'nobody', 'bed', 'war', 'seventeen', 'share', 'stand', 'family', 'deep', 'chance', 'everyone', 'forget', 'trust', 'mom', 'living', 'mercy', 'hate', 'held', 'someday', 'stuck', 'dream', 'room', 'disappear', 'somebody', 'days', 'letter', 'loves', 'men', 'peace', 'become', 'course']\n",
      "\n",
      "ðŸ“ CLUSTER 2 (Sample: 1355)\n",
      "   ðŸ·ï¸ Saved Label: ('urban_environment', 0.9)\n",
      "   Keywords: ['fight', 'men', 'war', 'captain', 'people', 'deserves', 'lives', 'mercy', 'win', 'home', 'rise', 'powerless', 'chance', 'storm', 'survive', 'freedom', 'buck', 'longer', 'land', 'raise', 'yes', 'woah', 'revolution', 'story', 'king', 'place', 'tonight', 'fall', 'greet', 'stand', 'left', 'arms', 'ruthlessness', 'peace', 'kill', 'ones', 'power', 'france', 'til', 'thing', 'nation', 'someday', 'burn', 'children', 'try', 'son', 'history', 'rest', 'teach', 'action', 'disappear', 'everyone', 'matter', 'sure', 'years', 'lead', 'glass', 'troops', 'command', 'pay', 'speed', 'team', 'fly', 'something', 'help', 'deep', 'alright', 'foes', 'push', 'plan', 'fine', 'forgotten', 'comrades', 'city', 'someone', 'meet', 'light', 'break', 'friends', 'south', 'vain', 'fade', 'half', 'count', 'sea', 'age', 'care', 'means', 'game', 'fought', 'school', 'sir', 'show', 'defeat', 'blow', 'work', 'cannot', 'town', 'cut', 'listen']\n",
      "\n",
      "ðŸ“ CLUSTER 3 (Sample: 1678)\n",
      "   ðŸ·ï¸ Saved Label: ('room_environment', 1.0)\n",
      "   Keywords: ['try', 'game', 'thing', 'fight', 'til', 'something', 'line', 'break', 'monster', 'work', 'home', 'longer', 'chance', 'power', 'help', 'play', 'shot', 'plan', 'wizard', 'someone', 'story', 'use', 'speed', 'write', 'thought', 'course', 'friend', 'test', 'sure', 'mistake', 'fine', 'stand', 'yes', 'lead', 'please', 'people', 'brain', 'cannot', 'mine', 'kill', 'turn', 'show', 'advice', 'hell', 'throw', 'set', 'worry', 'idea', 'hurt', 'somebody', 'sir', 'mistakes', 'sorry', 'gravity', 'ground', 'problem', 'check', 'guess', 'blood', 'pay', 'player', 'brother', 'mean', 'deep', 'step', 'understand', 'throwin', 'doubt', 'anything', 'left', 'prove', 'alright', 'learn', 'rules', 'trust', 'place', 'climb', 'ask', 'walk', 'fix', 'lot', 'lives', 'attack', 'broken', 'taste', 'kind', 'trick', 'deed', 'breaks', 'survive', 'dream', 'guard', 'okay', 'point', 'matter', 'helpless', 'deal', 'sense', 'luck', 'choose']\n",
      "\n",
      "ðŸ“ CLUSTER 4 (Sample: 634)\n",
      "   ðŸ·ï¸ Saved Label: ('sky_environment', 1.0)\n",
      "   Keywords: ['soldier', 'president', 'alexander', 'politician', 'washington', 'sir', 'york', 'thomas', 'home', 'congress', 'secretary', 'side', 'madison', 'plan', 'nation', 'george', 'state', 'friend', 'government', 'john', 'stand', 'adams', 'father', 'aaron', 'james', 'immigrant', 'choose', 'treasury', 'speech', 'dear', 'write', 'revolution', 'someone', 'history', 'lafayette', 'schuyler', 'men', 'mean', 'thing', 'letter', 'constitution', 'debt', 'reynolds', 'westerberg', 'lot', 'talk', 'meet', 'speak', 'yes', 'honest', 'fight', 'son', 'climb', 'essays', 'war', 'welcome', 'seat', 'room', 'power', 'people', 'money', 'project', 'teach', 'story', 'grow', 'law', 'senate', 'floor', 'response', 'walk', 'hear', 'somebody', 'guy', 'fall', 'manhattan', 'tryin', 'ask', 'job', 'door', 'convention', 'read', 'debts', 'thank', 'votes', 'hate', 'dinner', 'capital', 'claims', 'matter', 'chance', 'king', 'action', 'vice', 'worry', 'number', 'remind', 'knows', 'guess', 'citizens', 'hero']\n",
      "\n",
      "ðŸ“ CLUSTER 5 (Sample: 1312)\n",
      "   ðŸ·ï¸ Saved Label: ('dark_environment', 1.0)\n",
      "   Keywords: ['room', 'light', 'sky', 'dream', 'happens', 'sun', 'storm', 'home', 'dark', 'place', 'felt', 'people', 'disappear', 'city', 'become', 'fire', 'stars', 'watch', 'shine', 'wind', 'feels', 'til', 'thought', 'fireworks', 'thing', 'past', 'space', 'burn', 'nobody', 'bright', 'cold', 'round', 'strange', 'days', 'pass', 'beautiful', 'summer', 'window', 'woah', 'dreams', 'story', 'upside', 'years', 'tonight', 'house', 'show', 'imagine', 'longer', 'rise', 'sleep', 'helpless', 'step', 'forest', 'someone', 'lights', 'fate', 'fear', 'everybody', 'island', 'monster', 'limit', 'something', 'comet', 'land', 'yes', 'heights', 'sure', 'everyone', 'colder', 'anybody', 'sight', 'heat', 'air', 'blind', 'blue', 'street', 'guess', 'deep', 'friend', 'minute', 'turn', 'somebody', 'rewind', 'universe', 'daylight', 'wan', 'knows', 'walls', 'whirl', 'bring', 'somehow', 'matter', 'ground', 'hear', 'cry', 'cannot', 'kind', 'memory', 'work', 'rest']\n",
      "\n",
      "ðŸ“ CLUSTER 6 (Sample: 1544)\n",
      "   ðŸ·ï¸ Saved Label: ('ocean_environment', 1.0)\n",
      "   Keywords: ['home', 'house', 'tonight', 'party', 'place', 'round', 'city', 'york', 'work', 'piragua', 'ninety', 'thousand', 'drink', 'fun', 'today', 'coffee', 'money', 'bag', 'something', 'okay', 'summer', 'school', 'friend', 'pop', 'thing', 'woah', 'heights', 'show', 'store', 'town', 'friends', 'guy', 'people', 'snowman', 'everybody', 'step', 'food', 'block', 'perfect', 'sweet', 'folks', 'build', 'mom', 'bottle', 'weekend', 'goin', 'champagne', 'sir', 'play', 'bit', 'please', 'drive', 'catch', 'check', 'sugar', 'butter', 'break', 'someone', 'ride', 'kids', 'bed', 'anything', 'fill', 'betelgeuse', 'flour', 'left', 'yes', 'wear', 'line', 'pay', 'watch', 'pants', 'lights', 'lot', 'brand', 'years', 'use', 'parents', 'hear', 'ask', 'downtown', 'cave', 'ice', 'alright', 'thank', 'pass', 'door', 'ends', 'thirty', 'try', 'doin', 'upstate', 'grab', 'cool', 'candy', 'eat', 'celebration', 'emerald', 'ground', 'cup']\n",
      "\n",
      "ðŸ“ CLUSTER 7 (Sample: 1218)\n",
      "   ðŸ·ï¸ Saved Label: ('fire_environment', 1.0)\n",
      "   Keywords: ['people', 'friends', 'everyone', 'someone', 'friend', 'school', 'thinks', 'guy', 'guess', 'somebody', 'person', 'mean', 'cool', 'talk', 'loser', 'guys', 'geek', 'care', 'kinda', 'hate', 'show', 'matter', 'likes', 'try', 'nobody', 'something', 'college', 'pretend', 'boys', 'trust', 'kind', 'suck', 'ask', 'smile', 'pants', 'bit', 'thing', 'play', 'act', 'fun', 'thought', 'room', 'everybody', 'sorry', 'left', 'men', 'woah', 'feelings', 'game', 'smartest', 'yes', 'happier', 'lame', 'belong', 'stand', 'felt', 'help', 'wonderful', 'attention', 'meant', 'sweet', 'seem', 'means', 'sucks', 'teach', 'wear', 'understand', 'mad', 'fine', 'sexy', 'sit', 'buddy', 'anything', 'fight', 'kid', 'simple', 'closer', 'puppeteer', 'side', 'share', 'become', 'hang', 'sure', 'anyone', 'throw', 'year', 'player', 'stare', 'party', 'notice', 'watch', 'shame', 'pride', 'dirty', 'miss', 'hide', 'strange', 'proud', 'fix', 'laugh']\n",
      "\n",
      "ðŸ“ CLUSTER 8 (Sample: 1173)\n",
      "   ðŸ·ï¸ Saved Label: ('snow_environment', 1.0)\n",
      "   Keywords: ['queen', 'student', 'beautiful', 'wife', 'eliza', 'sister', 'mom', 'mother', 'friend', 'witch', 'mine', 'guy', 'power', 'abuela', 'hear', 'angelica', 'home', 'george', 'caw', 'fight', 'dream', 'miss', 'hair', 'christine', 'lydia', 'show', 'side', 'nina', 'something', 'bring', 'magic', 'story', 'read', 'kind', 'friends', 'party', 'barbara', 'creepy', 'mean', 'kill', 'yes', 'penelope', 'hurt', 'help', 'elsa', 'ignore', 'janis', 'revenge', 'walkin', 'people', 'body', 'jenna', 'bed', 'tells', 'sorry', 'please', 'ask', 'brooke', 'room', 'eat', 'beat', 'taste', 'fall', 'course', 'nessa', 'galinda', 'watch', 'needs', 'play', 'martha', 'aaron', 'door', 'moment', 'anything', 'knows', 'child', 'sure', 'hell', 'woah', 'bride', 'tonight', 'choose', 'gretchen', 'cow', 'suicide', 'deep', 'years', 'care', 'fear', 'water', 'cat', 'heard', 'place', 'rolan', 'person', 'helpless', 'schuyler', 'thinkin', 'anyone', 'alexander']\n",
      "\n",
      "ðŸ“ CLUSTER 9 (Sample: 1056)\n",
      "   ðŸ·ï¸ Saved Label: ('room_environment', 0.9)\n",
      "   Keywords: ['guy', 'set', 'fire', 'son', 'friend', 'house', 'michael', 'dad', 'father', 'daddy', 'jeremy', 'bathroom', 'home', 'hair', 'left', 'monster', 'heard', 'something', 'party', 'become', 'creepy', 'dream', 'bit', 'woah', 'fight', 'thing', 'thought', 'kill', 'voya', 'side', 'send', 'mother', 'someone', 'husband', 'yes', 'please', 'kid', 'people', 'weird', 'kind', 'captain', 'show', 'looks', 'break', 'knows', 'til', 'lot', 'help', 'buck', 'mine', 'sleep', 'odysseus', 'teach', 'friends', 'bed', 'fixer', 'game', 'hear', 'pride', 'chance', 'wife', 'prince', 'wizard', 'talk', 'lesson', 'dude', 'drink', 'room', 'daughter', 'release', 'sees', 'wonderful', 'money', 'awesome', 'sort', 'runs', 'fine', 'listen', 'loser', 'drunk', 'learn', 'everybody', 'king', 'lee', 'attack', 'men', 'mean', 'street', 'somebody', 'soldier', 'strike', 'trust', 'cyclops', 'bones', 'try', 'today', 'steal', 'lord', 'town', 'john']\n"
     ]
    }
   ],
   "source": [
    "# 1. CORRECT WAY TO LOAD\n",
    "with open('output/pickle/kmeans_presets(musical_and_pop).pkl', 'rb') as f:\n",
    "    kk = pickle.load(f)\n",
    "\n",
    "with open('output/pickle/preset_labels(musical_and_pop).pkl', 'rb') as f:\n",
    "    label_map = pickle.load(f)\n",
    "    \n",
    "# Check the difference between your current text and your saved brain\n",
    "print(f\"ðŸ“‰ Current Corpus Length: {len(corpus)}\")\n",
    "print(f\"ðŸ§  Saved Model Labels:    {len(kk.labels_)}\")\n",
    "\n",
    "# 2. RUN YOUR ANALYSIS\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"   YOUR VISUAL SHOPPING LIST\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# âš ï¸ CRITICAL NOTE: 'corpus' must exist in memory!\n",
    "# If this is a new script, you need to re-generate or load the 'corpus' list\n",
    "# exactly as it was during training, otherwise model.labels_ won't match.\n",
    "if 'corpus' not in locals():\n",
    "    print(\"âŒ ERROR: Variable 'corpus' is missing. You need the text data to analyze keywords.\")\n",
    "else:\n",
    "    df = pd.DataFrame({'text': corpus, 'cluster': kk.labels_})\n",
    "\n",
    "    # Use NUM_CLUSTERS from the loaded model to be safe\n",
    "    num_clusters = kk.n_clusters\n",
    "\n",
    "    for i in range(num_clusters):\n",
    "        cluster_text = df[df['cluster'] == i]['text'].tolist()\n",
    "        \n",
    "        # Use your custom filter function\n",
    "        keywords = get_top_keywords_filtered(cluster_text, 100)\n",
    "        \n",
    "        # Get the label you saved (if available) to verify\n",
    "        saved_label = label_map.get(i, \"Unknown\")\n",
    "        \n",
    "        print(f\"\\nðŸ“ CLUSTER {i} (Sample: {len(cluster_text)})\")\n",
    "        print(f\"   ðŸ·ï¸ Saved Label: {saved_label}\") \n",
    "        print(f\"   Keywords: {[k[0] for k in keywords]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-20T05:01:12.123374Z",
     "start_time": "2026-01-20T05:01:11.144765Z"
    }
   },
   "id": "60001e479b8d0cd4"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "preset_map = {\n",
    "    0: (\"urban_environment\", 1.0),\n",
    "    1: (\"urban_environment\", 0.8),\n",
    "    2: (\"urban_environment\", 0.9),\n",
    "    3: (\"room_environment\", 1.0),\n",
    "    4: (\"sky_environment\", 1.0),\n",
    "    5: (\"dark_environment\", 1.0),\n",
    "    6: (\"ocean_environment\", 1.0),\n",
    "    7: (\"fire_environment\", 1.0),\n",
    "    8: (\"snow_environment\", 1.0),\n",
    "    9: (\"room_environment\", 0.9)\n",
    "}\n",
    "\n",
    "with open('output/pickle/preset_labels(musical_and_pop).pkl', 'wb') as f:\n",
    "    pickle.dump(preset_map, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-19T06:55:47.597328Z",
     "start_time": "2026-01-19T06:55:47.594808Z"
    }
   },
   "id": "af46d1257b94400f"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File restored successfully!\n"
     ]
    }
   ],
   "source": [
    "musical_data.to_csv(\"dataset/musical.csv\", index=False)\n",
    "\n",
    "print(\"File restored successfully!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-19T07:29:56.932570Z",
     "start_time": "2026-01-19T07:29:56.890667Z"
    }
   },
   "id": "1f0402296000296e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "84b74add19411774"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
