{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-11T08:44:38.361074Z",
     "start_time": "2026-01-11T08:44:38.353653Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def cluster_cohesion(embeddings, labels):\n",
    "    scores = []\n",
    "    for c in np.unique(labels):\n",
    "        idx = labels == c\n",
    "        if idx.sum() < 2:\n",
    "            continue\n",
    "        sims = cosine_similarity(embeddings[idx])\n",
    "        scores.append(sims.mean())\n",
    "    return float(np.mean(scores))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T08:44:40.373942Z",
     "start_time": "2026-01-11T08:44:40.368339Z"
    }
   },
   "id": "4da65c4856278690"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8674b11a076872cc",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T08:44:41.525539Z",
     "start_time": "2026-01-11T08:44:41.485578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning and Segmenting Lyrics...\n",
      "âœ… Processing complete. Total segments: 10057\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"output/songs_with_lyrics_cleaned.csv\")\n",
    "separated_lyrics = data['lyrics'].dropna()\n",
    "\n",
    "# musical_data = pd.read_csv(\"dataset/musical.csv\")\n",
    "# separated_lyrics = musical_data['Lyrics'].dropna()\n",
    "# separated_lyrics = pd.concat([separated_lyrics, musical_data['Lyrics'].dropna()])\n",
    "\n",
    "corpus = []\n",
    "\n",
    "print(\"ðŸ§¹ Cleaning and Segmenting Lyrics...\")\n",
    "\n",
    "for lyric_text in separated_lyrics:\n",
    "    if isinstance(lyric_text, str):\n",
    "        # 1. Clean Tags\n",
    "        text_no_tags = re.sub(r'\\[.*?\\]', ' ', lyric_text, flags=re.DOTALL)\n",
    "        \n",
    "        # 2. Split into lines\n",
    "        raw_segments = text_no_tags.split('\\n')\n",
    "        \n",
    "        # 3. Clean and Filter\n",
    "        clean_lines = [line.strip() for line in raw_segments if len(line.strip()) > 0]\n",
    "        \n",
    "        # 4. THE MAGIC TRICK: \"Sliding Window\" of 2 lines\n",
    "        # We combine Line 1+2, then Line 2+3, then Line 3+4...\n",
    "        for i in range(len(clean_lines) - 1):\n",
    "            # Combine current line and next line\n",
    "            combined_segment = f\"{clean_lines[i]} {clean_lines[i+1]}\"\n",
    "\n",
    "            # Only keep if it's long enough (e.g. > 40 chars)\n",
    "            if len(combined_segment) > 40:\n",
    "                corpus.append(combined_segment)\n",
    "\n",
    "print(f\"âœ… Processing complete. Total segments: {len(corpus)}\")\n",
    "# Now 'corpus' is ready for SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4387bb0afe28e96a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T08:44:42.724051Z",
     "start_time": "2026-01-11T08:44:42.602051Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/Bay_Techatham/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/Bay_Techatham/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Bay_Techatham/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def get_top_keywords_filtered(texts, n=10):\n",
    "    # 1. EXPANDED STOPWORDS (Grammar + Slang)\n",
    "    custom_stops = list(nltk.corpus.stopwords.words('english'))\n",
    "    pop_slang = [\n",
    "        \"lil\", \"gon\", \"bout\", \"em\", \"ayy\", \"uh\", \"huh\", \"ha\", \"vi\", \"doo\", \"wee\",\n",
    "        \"yeah\", \"oh\", \"baby\", \"know\", \"like\", \"got\", \"just\", \"don\", \"ve\", \"ll\", \n",
    "        \"want\", \"need\", \"love\", \"time\", \"way\", \"make\", \"say\", \"come\", \"go\", \"right\",\n",
    "        \"look\", \"good\", \"feel\", \"really\", \"cause\", \"wanna\", \"gonna\", \"gotta\", \"ain\",\n",
    "        \"girl\", \"boy\", \"man\", \"woman\", \"hey\", \"ooh\", \"whoa\", \"shit\", \"fuck\", \"bitch\",\n",
    "        \"nigga\", \"niggas\", \"damn\", \"ass\", \"tell\", \"think\", \"never\", \"back\", \"let\",\n",
    "        \"swag\", \"yuh\", \"hum\", \"who\", \"what\", \"where\", \"why\", \"top\", \"call\", \"put\",\n",
    "        \"gang\", \"thug\", \"bro\", \"pussy\", \"tryna\", \"chick\", \"girls\", \"slatt\", \"mmh\"\n",
    "    ]\n",
    "    \n",
    "    # 2. ABSTRACT NOUNS BLACKLIST (The \"Non-Visual\" Filter)\n",
    "    # These are nouns/verbs that appear often but you can't build them in 3D.\n",
    "    abstract_concepts = [\n",
    "        \"life\", \"day\", \"night\", \"heart\", \"mind\", \"world\", \"everything\", \"nothing\", \n",
    "        \"things\", \"nothin\", \"songs\", \"song\", \"name\", \"eyes\", \"face\", \"voice\", \n",
    "        \"head\", \"hand\", \"hands\", \"god\", \"soul\", \"mind\", \"pain\", \"hope\", \"wish\", \n",
    "        \"fame\", \"lie\", \"lies\", \"truth\", \"word\", \"words\", \"end\", \"reason\", \"part\",\n",
    "        \"told\", \"saw\", \"knew\", \"met\", \"said\", \"made\", \"found\", \"came\", \"went\", # Past tense verbs\n",
    "        \"die\", \"born\", \"live\", \"dead\", \"death\", \"control\", \"move\", \"wait\", \"hold\",\n",
    "        \"stop\", \"start\", \"change\", \"keep\", \"leave\", \"stay\", \"believe\", \"remember\"\n",
    "    ]\n",
    "\n",
    "    # Combine all lists\n",
    "    all_stops = custom_stops + pop_slang + abstract_concepts\n",
    "\n",
    "    # 3. VECTORIZE\n",
    "    vec = CountVectorizer(stop_words=all_stops).fit(texts)\n",
    "    bag_of_words = vec.transform(texts)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    \n",
    "    # 4. FINAL CLEANUP (Noun Check + Length Check)\n",
    "    cleaned_list = []\n",
    "    for word, freq in sorted(words_freq, key=lambda x: x[1], reverse=True):\n",
    "        if len(word) <= 2: continue # Remove short garbage\n",
    "        \n",
    "        # Verify it's a Noun (NN)\n",
    "        pos_tag = nltk.pos_tag([word])[0][1]\n",
    "        if pos_tag.startswith('NN'):\n",
    "            cleaned_list.append((word, freq))\n",
    "            \n",
    "    return cleaned_list[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98d4fdd72a410d18",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T17:21:33.140851Z",
     "start_time": "2026-01-11T17:15:20.097697Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-12 02:15:20,096] A new study created in memory with name: no-name-39618001-509f-482e-8413-a21059baa870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Starting Optuna Optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-12 02:15:37,715] Trial 0 finished with value: 0.2716422975063324 and parameters: {'n_clusters': 48}. Best is trial 0 with value: 0.2716422975063324.\n",
      "[I 2026-01-12 02:15:52,311] Trial 1 finished with value: 0.2559882402420044 and parameters: {'n_clusters': 41}. Best is trial 0 with value: 0.2716422975063324.\n",
      "[I 2026-01-12 02:16:12,704] Trial 2 finished with value: 0.27651482820510864 and parameters: {'n_clusters': 59}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:16:31,916] Trial 3 finished with value: 0.27528074383735657 and parameters: {'n_clusters': 60}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:16:51,851] Trial 4 finished with value: 0.27528074383735657 and parameters: {'n_clusters': 60}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:17:09,412] Trial 5 finished with value: 0.266170859336853 and parameters: {'n_clusters': 50}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:17:26,276] Trial 6 finished with value: 0.26966679096221924 and parameters: {'n_clusters': 53}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:17:40,633] Trial 7 finished with value: 0.258674681186676 and parameters: {'n_clusters': 40}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:17:58,040] Trial 8 finished with value: 0.2759794294834137 and parameters: {'n_clusters': 58}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:18:14,909] Trial 9 finished with value: 0.26997604966163635 and parameters: {'n_clusters': 51}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:18:33,948] Trial 10 finished with value: 0.2706632912158966 and parameters: {'n_clusters': 55}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:18:54,381] Trial 11 finished with value: 0.2712375521659851 and parameters: {'n_clusters': 56}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:19:18,189] Trial 12 finished with value: 0.27605685591697693 and parameters: {'n_clusters': 57}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:19:34,513] Trial 13 finished with value: 0.25920554995536804 and parameters: {'n_clusters': 45}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:19:53,193] Trial 14 finished with value: 0.2712375521659851 and parameters: {'n_clusters': 56}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:20:11,924] Trial 15 finished with value: 0.2759794294834137 and parameters: {'n_clusters': 58}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:20:36,230] Trial 16 finished with value: 0.2678925395011902 and parameters: {'n_clusters': 54}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:20:55,547] Trial 17 finished with value: 0.2759794294834137 and parameters: {'n_clusters': 58}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:21:14,681] Trial 18 finished with value: 0.26942986249923706 and parameters: {'n_clusters': 52}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:21:33,119] Trial 19 finished with value: 0.2622126638889313 and parameters: {'n_clusters': 47}. Best is trial 2 with value: 0.27651482820510864.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "ðŸ† Best K found: 59\n",
      "ðŸ“ˆ Best Silhouette Score: 0.2765\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 0. ENSURE EMBEDDINGS EXIST\n",
    "if 'embeddings' not in globals():\n",
    "    print(\"â³ Generating Embeddings (this happens once)...\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(corpus)\n",
    "\n",
    "\n",
    "# 1. DEFINE THE OBJECTIVE FUNCTION\n",
    "# Optuna will run this function many times with different 'k' values\n",
    "def objective(trial):\n",
    "    k = trial.suggest_int('n_clusters', 40, 60)\n",
    "\n",
    "    model = KMeans(\n",
    "        n_clusters=k,\n",
    "        random_state=42,\n",
    "        n_init=10\n",
    "    )\n",
    "    labels = model.fit_predict(embeddings)\n",
    "\n",
    "    cohesion = cluster_cohesion(embeddings, labels)\n",
    "\n",
    "    return cohesion  # HIGHER = better semantic cohesion\n",
    "\n",
    "# 2. RUN THE OPTIMIZATION\n",
    "print(\"ðŸ¤– Starting Optuna Optimization...\")\n",
    "study = optuna.create_study(direction='maximize')  # We want MAX separation\n",
    "study.optimize(objective, n_trials=20)  # Try 20 different times\n",
    "\n",
    "# 3. PRINT RESULTS\n",
    "print(\"-\" * 40)\n",
    "print(f\"ðŸ† Best K found: {study.best_params['n_clusters']}\")\n",
    "print(f\"ðŸ“ˆ Best Silhouette Score: {study.best_value:.4f}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 4. OPTIONAL: VISUALIZE\n",
    "optuna.visualization.plot_optimization_history(study).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e39b6d9ca8388e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T17:22:36.855566Z",
     "start_time": "2026-01-11T17:22:21.399976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§  Analyzing 10057 lyric segments with SBERT...\n",
      "\n",
      "========================================\n",
      "   YOUR VISUAL SHOPPING LIST\n",
      "========================================\n",
      "\n",
      "ðŸ“ CLUSTER 1 (Sample: 118)\n",
      "   Keywords: ['lookin', 'mirror', 'missin', 'wind', 'looks', 'hate', 'halfway', 'yes', 'vision', 'dress']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 2 (Sample: 178)\n",
      "   Keywords: ['left', 'times', 'quit', 'care', 'hate', 'half', 'miss', 'home', 'chaos', 'fall']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 3 (Sample: 204)\n",
      "   Keywords: ['cry', 'broken', 'sad', 'broke', 'tears', 'heartbreak', 'hurt', 'break', 'vacancy', 'sorry']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 4 (Sample: 414)\n",
      "   Keywords: ['masa', 'trim', 'homie', 'money', 'hoes', 'pull', 'lot', 'chaos', 'rap', 'playin']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 5 (Sample: 303)\n",
      "   Keywords: ['hate', 'care', 'fuckin', 'treat', 'hurt', 'thought', 'sorry', 'okay', 'blame', 'thank']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 6 (Sample: 127)\n",
      "   Keywords: ['goodbye', 'lives', 'state', 'bettin', 'roll', 'dice', 'hear', 'survival', 'suicidal', 'bodies']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 7 (Sample: 29)\n",
      "   Keywords: ['stand', 'crowd', 'ones']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 8 (Sample: 287)\n",
      "   Keywords: ['money', 'pay', 'cash', 'dollar', 'spend', 'paid', 'price', 'twelve', 'dime', 'watch']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 9 (Sample: 176)\n",
      "   Keywords: ['king', 'disorder', 'silence', 'phantom', 'sleep', 'island', 'demon', 'insane', 'asylum', 'restless']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 10 (Sample: 178)\n",
      "   Keywords: ['violence', 'woah', 'wop', 'hunnid', 'viol', 'shake', 'kanayo', 'matter', 'woo', 'moo']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 11 (Sample: 349)\n",
      "   Keywords: ['side', 'crib', 'ride', 'friends', 'rush', 'dog', 'room', 'mutt', 'hit', 'cool']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 12 (Sample: 67)\n",
      "   Keywords: ['thriller', 'tonight', 'killer', 'thrill', 'nights', 'summer', 'callin', 'midnightin', 'backslidin', 'thing']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 13 (Sample: 153)\n",
      "   Keywords: ['dance', 'motion', 'walk', 'fearless', 'body', 'fall', 'feet', 'stand', 'tall', 'drag']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 14 (Sample: 97)\n",
      "   Keywords: ['christmas', 'everyday', 'gift', 'angel', 'side', 'lovin', 'year', 'holiday', 'home', 'tonight']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 15 (Sample: 151)\n",
      "   Keywords: ['sleep', 'bed', 'woah', 'wake', 'morning', 'room', 'nights', 'dream', 'awake', 'sheets']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 16 (Sample: 213)\n",
      "   Keywords: ['help', 'break', 'win', 'play', 'try', 'kill', 'body', 'hurry', 'leaf', 'round']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 17 (Sample: 184)\n",
      "   Keywords: ['blue', 'jean', 'home', 'kinda', 'heaven', 'rosie', 'times', 'left', 'california', 'party']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 18 (Sample: 205)\n",
      "   Keywords: ['water', 'sea', 'something', 'rain', 'drown', 'waves', 'blood', 'deep', 'storm', 'tears']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 19 (Sample: 270)\n",
      "   Keywords: ['motherfuckin', 'starboy', 'meier', 'home', 'cowboy', 'leather', 'school', 'shame', 'funny', 'murder']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 20 (Sample: 148)\n",
      "   Keywords: ['talk', 'speak', 'hear', 'silence', 'friends', 'naw', 'chat', 'hearing', 'sayin', 'please']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 21 (Sample: 193)\n",
      "   Keywords: ['wherever', 'traveller', 'belong', 'greetings', 'hometown', 'mount', 'zion', 'honey', 'place', 'country']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 22 (Sample: 85)\n",
      "   Keywords: ['money', 'friends', 'mad', 'sexy', 'rock', 'fashions', 'set', 'trends', 'hate', 'crazy']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 23 (Sample: 37)\n",
      "   Keywords: ['maleante', 'con', 'caballeros', 'cuban', 'links', 'step', 'perico', 'cartel', 'feliz', 'navidad']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 24 (Sample: 160)\n",
      "   Keywords: ['sun', 'sunrise', 'california', 'midnight', 'til', 'stars', 'sky', 'shines', 'fade', 'trees']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 25 (Sample: 198)\n",
      "   Keywords: ['skin', 'beneath', 'everyone', 'hide', 'eye', 'looks', 'lookin', 'deeper', 'fly', 'body']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 26 (Sample: 28)\n",
      "   Keywords: ['lucy', 'sky', 'diamonds', 'kaleidoscope']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 27 (Sample: 167)\n",
      "   Keywords: ['feels', 'wild', 'bit', 'city', 'left', 'felt', 'peculiar', 'belong', 'stuck', 'alright']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 28 (Sample: 245)\n",
      "   Keywords: ['body', 'kiss', 'yes', 'thank', 'belong', 'dance', 'fallin', 'crazy', 'shape', 'somethin']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 29 (Sample: 196)\n",
      "   Keywords: ['drink', 'beer', 'drinkin', 'hungover', 'bourbon', 'drunk', 'whiskey', 'cold', 'wine', 'home']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 30 (Sample: 73)\n",
      "   Keywords: ['sickness', 'sick', 'violence', 'violent', 'nature', 'motherfucker', 'medicine', 'taste', 'hate', 'flow']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 31 (Sample: 198)\n",
      "   Keywords: ['eminem', 'feat', 'hits', 'earl', 'davis', 'kanye', 'west', 'tyler', 'creator', 'money']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 32 (Sample: 172)\n",
      "   Keywords: ['hearts', 'stole', 'peace', 'earth', 'joy', 'everyone', 'home', 'texas', 'cry', 'deep']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 33 (Sample: 276)\n",
      "   Keywords: ['kept', 'left', 'door', 'sweet', 'feelin', 'thought', 'home', 'friends', 'nobody', 'crazy']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 34 (Sample: 340)\n",
      "   Keywords: ['karat', 'magic', 'air', 'cut', 'rings', 'patterns', 'diamonds', 'stick', 'sucks', 'suck']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 35 (Sample: 29)\n",
      "   Keywords: ['taylor', 'swift', 'reputation', 'swifts', 'talk', 'winner', 'address', 'rumor', 'superstar', 'photographers']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 36 (Sample: 84)\n",
      "   Keywords: ['coupe', 'lotta', 'thot', 'pint', 'blue', 'motherfuckin', 'shot', 'throwin', 'burning', 'sense']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 37 (Sample: 238)\n",
      "   Keywords: ['road', 'walk', 'ride', 'drive', 'turn', 'highway', 'car', 'lonesome', 'drifter', 'traveller']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 38 (Sample: 280)\n",
      "   Keywords: ['pushin', 'hit', 'gettin', 'goin', 'sayin', 'tryin', 'fallin', 'bang', 'thing', 'thÐµ']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 39 (Sample: 142)\n",
      "   Keywords: ['none', 'confess', 'sorry', 'demons', 'trust', 'someone', 'something', 'cousin', 'swear', 'lover']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 40 (Sample: 129)\n",
      "   Keywords: ['fire', 'burn', 'blow', 'firecracker', 'burning', 'light', 'flames', 'turn', 'thought', 'burns']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 41 (Sample: 141)\n",
      "   Keywords: ['heaven', 'praise', 'rehearsal', 'test', 'flight', 'hee', 'sky', 'pray', 'flies', 'prayin']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 42 (Sample: 262)\n",
      "   Keywords: ['try', 'everybody', 'fight', 'hate', 'days', 'thing', 'blame', 'hell', 'stuck', 'learn']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 43 (Sample: 197)\n",
      "   Keywords: ['lover', 'drugs', 'bit', 'deep', 'suitcases', 'somebody', 'everyday', 'blue', 'fall', 'thought']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 44 (Sample: 60)\n",
      "   Keywords: ['doin', 'shame', 'champagne', 'taste', 'endless', 'fortune', 'hall', 'hollywood', 'something', 'hallway']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 45 (Sample: 311)\n",
      "   Keywords: ['fuckin', 'bitches', 'cartier', 'shoot', 'shot', 'gun', 'glock', 'pull', 'front', 'ride']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 46 (Sample: 123)\n",
      "   Keywords: ['cry', 'eat', 'tears', 'mama', 'care', 'drown', 'mouth', 'praise', 'tip', 'tongue']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 47 (Sample: 187)\n",
      "   Keywords: ['party', 'town', 'city', 'house', 'home', 'shake', 'par', 'friends', 'sticks', 'place']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 48 (Sample: 171)\n",
      "   Keywords: ['bells', 'jingle', 'sound', 'ride', 'fun', 'row', 'choir', 'hear', 'sings', 'chime']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 49 (Sample: 27)\n",
      "   Keywords: ['ten', 'power', 'size', 'twenty', 'cluster', 'zettametre', 'yottametre', 'super', 'megametre', 'twelve']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 50 (Sample: 104)\n",
      "   Keywords: ['snow', 'sleigh', 'reindeer', 'rudolph', 'horse', 'christmas', 'angel', 'mine', 'glows', 'bells']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 51 (Sample: 86)\n",
      "   Keywords: ['christmas', 'merry', 'bottom', 'year', 'thank', 'ghost', 'years', 'spend', 'tree', 'offering']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 52 (Sample: 266)\n",
      "   Keywords: ['heaven', 'amen', 'pray', 'father', 'hell', 'jesus', 'hallelujah', 'vow', 'glory', 'peace']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 53 (Sample: 125)\n",
      "   Keywords: ['living', 'dream', 'vow', 'fall', 'breaker', 'tomorrow', 'hear', 'tonight', 'moment', 'rock']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 54 (Sample: 160)\n",
      "   Keywords: ['light', 'cherry', 'bomb', 'dark', 'lights', 'stars', 'homie', 'imma', 'bright', 'kinda']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 55 (Sample: 165)\n",
      "   Keywords: ['times', 'memory', 'days', 'yesterday', 'carry', 'memories', 'turn', 'show', 'moment', 'past']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 56 (Sample: 20)\n",
      "   Keywords: ['hood', 'hunnid', 'thou', 'rollin', 'ridin', 'past', 'goin', 'fast']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 57 (Sample: 208)\n",
      "   Keywords: ['meet', 'door', 'yes', 'try', 'decide', 'number', 'someone', 'thanks', 'phone', 'church']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 58 (Sample: 219)\n",
      "   Keywords: ['hurry', 'times', 'peace', 'carry', 'alienation', 'nation', 'bidness', 'tender', 'hearts', 'chances']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 59 (Sample: 104)\n",
      "   Keywords: ['breath', 'breathe', 'tight', 'deep', 'chest', 'breeze', 'breaths', 'lovin', 'whirlwind', 'yes']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nðŸ§  Analyzing {len(corpus)} lyric segments with SBERT...\")\n",
    "\n",
    "NUM_CLUSTERS = 59\n",
    "\n",
    "# A. Embed\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(corpus)\n",
    "\n",
    "# B. Cluster\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42)\n",
    "kmeans.fit(embeddings)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# ==========================================\n",
    "# 4. RESULTS (THE \"SHOPPING LIST\")\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"   YOUR VISUAL SHOPPING LIST\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Helper to find words from the clusters\n",
    "def get_top_keywords(texts, n=8):\n",
    "    vec = CountVectorizer(stop_words='english').fit(texts)\n",
    "    bag_of_words = vec.transform(texts)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    return sorted(words_freq, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "df = pd.DataFrame({'text': corpus, 'cluster': cluster_labels})\n",
    "\n",
    "for i in range(NUM_CLUSTERS):\n",
    "    cluster_text = df[df['cluster'] == i]['text'].tolist()\n",
    "    \n",
    "    # Use the new function\n",
    "    keywords = get_top_keywords_filtered(cluster_text)\n",
    "    \n",
    "    print(f\"\\nðŸ“ CLUSTER {i+1} (Sample: {len(cluster_text)})\")\n",
    "    # Print just the words, not the frequency counts\n",
    "    print(f\"   Keywords: {[k[0] for k in keywords]}\")\n",
    "    print(f\"   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4f5bfedd4f5e412",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T09:01:55.441029Z",
     "start_time": "2026-01-11T09:01:55.423297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved as 'kmeans_presets.pkl'\n",
      "âœ… Labels saved as 'preset_labels.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# 1. SAVE THE MODEL\n",
    "# (Make sure 'kmeans' is your trained model variable)\n",
    "# Using joblib is often safer for sklearn models than pickle, but pickle works too.\n",
    "with open('kmeans_presets.pkl', 'wb') as f:\n",
    "    pickle.dump(kmeans, f)\n",
    "print(\"âœ… Model saved as 'kmeans_presets.pkl'\")\n",
    "\n",
    "# 2. SAVE YOUR ASSET MAP\n",
    "# Mapped to match your SBERT Keywords exactly:\n",
    "\n",
    "preset_map = {\n",
    "    0: \"urban_environment\",  # Cluster 1: Money, Piragua, Bag (In The Heights/City)\n",
    "    1: \"urban_environment\",  # Cluster 2: Dance, Shake, Shot (Party/Club Vibe)\n",
    "    2: \"urban_environment\",  # Cluster 3: Heather, Regina, Queen (High School/Social)\n",
    "    3: \"room_environment\",   # Cluster 4: Home, Door, Walk (Domestic/Inside)\n",
    "    4: \"sky_environment\",    # Cluster 5: Sun, Light, Sky, Stars (Lanterns/Magic)\n",
    "    5: \"dark_environment\",   # Cluster 6: Hate, Fight, Care (Conflict/Spooky)\n",
    "    6: \"ocean_environment\",  # Cluster 7: Captain, War, Men, Stand (Epic/Sea)\n",
    "    7: \"fire_environment\",   # Cluster 8: Hamilton, Fire, Burn (Revolution/Rage)\n",
    "    8: \"snow_environment\",   # Cluster 9: Christmas, Merry, Fall (Winter)\n",
    "    9: \"room_environment\"    # Cluster 10: Room, Dream, Sleep (Intimate/Thoughts)\n",
    "}\n",
    "\n",
    "with open('preset_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(preset_map, f)\n",
    "print(\"âœ… Labels saved as 'preset_labels.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§  Analyzing 10057 lyric segments with SBERT...\n",
      "\n",
      "========================================\n",
      "   MAIN STAGE BASED ON RUSSELL'S\n",
      "========================================\n",
      "\n",
      "ðŸ“ CLUSTER 1 (Sample: 3099)\n",
      "   Keywords: ['money', 'hit', 'doin', 'play', 'fuckin', 'ride', 'home', 'hunnid', 'lot', 'round']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 2 (Sample: 841)\n",
      "   Keywords: ['taylor', 'ride', 'eat', 'side', 'swift', 'crib', 'friends', 'mama', 'reputation', 'blue']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 3 (Sample: 3347)\n",
      "   Keywords: ['christmas', 'cry', 'hate', 'try', 'walk', 'care', 'home', 'dance', 'body', 'break']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 4 (Sample: 2770)\n",
      "   Keywords: ['sun', 'christmas', 'lucy', 'light', 'heaven', 'sky', 'fire', 'stand', 'king', 'snow']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nðŸ§  Analyzing {len(corpus)} lyric segments with SBERT...\")\n",
    "\n",
    "NUM_CLUSTERS = 4\n",
    "\n",
    "# A. Embed\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(corpus)\n",
    "\n",
    "# B. Cluster\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42)\n",
    "kmeans.fit(embeddings)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# ==========================================\n",
    "# 4. RESULTS (THE \"SHOPPING LIST\")\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"   MAIN STAGE BASED ON RUSSELL'S\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Helper to find words from the clusters\n",
    "def get_top_keywords(texts, n=8):\n",
    "    vec = CountVectorizer(stop_words='english').fit(texts)\n",
    "    bag_of_words = vec.transform(texts)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    return sorted(words_freq, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "df = pd.DataFrame({'text': corpus, 'cluster': cluster_labels})\n",
    "\n",
    "for i in range(NUM_CLUSTERS):\n",
    "    cluster_text = df[df['cluster'] == i]['text'].tolist()\n",
    "    \n",
    "    # Use the new function\n",
    "    keywords = get_top_keywords_filtered(cluster_text)\n",
    "    \n",
    "    print(f\"\\nðŸ“ CLUSTER {i+1} (Sample: {len(cluster_text)})\")\n",
    "    # Print just the words, not the frequency counts\n",
    "    print(f\"   Keywords: {[k[0] for k in keywords]}\")\n",
    "    print(f\"   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T18:04:28.519312Z",
     "start_time": "2026-01-11T18:04:15.162457Z"
    }
   },
   "id": "60001e479b8d0cd4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "af46d1257b94400f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
