{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-11T08:44:38.361074Z",
     "start_time": "2026-01-11T08:44:38.353653Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def cluster_cohesion(embeddings, labels):\n",
    "    scores = []\n",
    "    for c in np.unique(labels):\n",
    "        idx = labels == c\n",
    "        if idx.sum() < 2:\n",
    "            continue\n",
    "        sims = cosine_similarity(embeddings[idx])\n",
    "        scores.append(sims.mean())\n",
    "    return float(np.mean(scores))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T08:44:40.373942Z",
     "start_time": "2026-01-11T08:44:40.368339Z"
    }
   },
   "id": "4da65c4856278690"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8674b11a076872cc",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-12T06:33:34.081474Z",
     "start_time": "2026-01-12T06:33:34.040171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning and Segmenting Lyrics...\n",
      "âœ… Processing complete. Total segments: 12767\n"
     ]
    }
   ],
   "source": [
    "# data = pd.read_csv(\"output/songs_with_lyrics_cleaned.csv\")\n",
    "# separated_lyrics = data['lyrics'].dropna()\n",
    "\n",
    "musical_data = pd.read_csv(\"dataset/musical.csv\")\n",
    "separated_lyrics = musical_data['Lyrics'].dropna()\n",
    "# separated_lyrics = pd.concat([separated_lyrics, musical_data['Lyrics'].dropna()])\n",
    "\n",
    "corpus = []\n",
    "\n",
    "print(\"ðŸ§¹ Cleaning and Segmenting Lyrics...\")\n",
    "\n",
    "for lyric_text in separated_lyrics:\n",
    "    if isinstance(lyric_text, str):\n",
    "        # 1. Clean Tags\n",
    "        text_no_tags = re.sub(r'\\[.*?\\]', ' ', lyric_text, flags=re.DOTALL)\n",
    "        \n",
    "        # 2. Split into lines\n",
    "        raw_segments = text_no_tags.split('\\n')\n",
    "        \n",
    "        # 3. Clean and Filter\n",
    "        clean_lines = [line.strip() for line in raw_segments if len(line.strip()) > 0]\n",
    "        \n",
    "        # 4. THE MAGIC TRICK: \"Sliding Window\" of 2 lines\n",
    "        # We combine Line 1+2, then Line 2+3, then Line 3+4...\n",
    "        for i in range(len(clean_lines) - 1):\n",
    "            # Combine current line and next line\n",
    "            combined_segment = f\"{clean_lines[i]} {clean_lines[i+1]}\"\n",
    "\n",
    "            # Only keep if it's long enough (e.g. > 40 chars)\n",
    "            if len(combined_segment) > 40:\n",
    "                corpus.append(combined_segment)\n",
    "\n",
    "print(f\"âœ… Processing complete. Total segments: {len(corpus)}\")\n",
    "# Now 'corpus' is ready for SBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Filter/replace words"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30734f98f95af4c9"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4387bb0afe28e96a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-12T05:47:59.163903Z",
     "start_time": "2026-01-12T05:47:59.034247Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/Bay_Techatham/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/Bay_Techatham/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Bay_Techatham/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download resources (run once)\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def get_top_keywords_filtered(texts, n=10):\n",
    "    # 1. SETUP LISTS\n",
    "    custom_stops = list(nltk.corpus.stopwords.words('english'))\n",
    "    pop_slang = [\n",
    "        \"lil\", \"gon\", \"bout\", \"em\", \"ayy\", \"uh\", \"huh\", \"ha\", \"vi\", \"doo\", \"wee\",\n",
    "        \"yeah\", \"oh\", \"baby\", \"know\", \"like\", \"got\", \"just\", \"don\", \"ve\", \"ll\", \n",
    "        \"want\", \"need\", \"love\", \"time\", \"way\", \"make\", \"say\", \"come\", \"go\", \"right\",\n",
    "        \"look\", \"good\", \"feel\", \"really\", \"cause\", \"wanna\", \"gonna\", \"gotta\", \"ain\",\n",
    "        \"girl\", \"boy\", \"man\", \"woman\", \"hey\", \"ooh\", \"whoa\", \"shit\", \"fuck\", \"bitch\",\n",
    "        \"nigga\", \"niggas\", \"damn\", \"ass\", \"tell\", \"think\", \"never\", \"back\", \"let\",\n",
    "        \"swag\", \"yuh\", \"hum\", \"who\", \"what\", \"where\", \"why\", \"top\", \"call\", \"put\",\n",
    "        \"gang\", \"thug\", \"bro\", \"pussy\", \"tryna\", \"chick\", \"girls\", \"slatt\", \"mmh\"\n",
    "    ]\n",
    "    \n",
    "    abstract_concepts = [\n",
    "        \"life\", \"day\", \"night\", \"heart\", \"mind\", \"world\", \"everything\", \"nothing\", \n",
    "        \"things\", \"nothin\", \"songs\", \"song\", \"name\", \"eyes\", \"face\", \"voice\", \n",
    "        \"head\", \"hand\", \"hands\", \"god\", \"soul\", \"mind\", \"pain\", \"hope\", \"wish\", \n",
    "        \"fame\", \"lie\", \"lies\", \"truth\", \"word\", \"words\", \"end\", \"reason\", \"part\",\n",
    "        \"told\", \"saw\", \"knew\", \"met\", \"said\", \"made\", \"found\", \"came\", \"went\",\n",
    "        \"die\", \"born\", \"live\", \"dead\", \"death\", \"control\", \"move\", \"wait\", \"hold\",\n",
    "        \"stop\", \"start\", \"change\", \"keep\", \"leave\", \"stay\", \"believe\", \"remember\"\n",
    "    ]\n",
    "    \n",
    "    NAME_REPLACEMENTS = {\n",
    "        \"regina\": \"queen\", \"veronica\": \"girl\", \"heather\": \"student\",\n",
    "        \"hamilton\": \"soldier\", \"burr\": \"man\", \"jefferson\": \"politician\",\n",
    "        \"elphaba\": \"witch\", \"glinda\": \"friend\", \"evan\": \"boy\",\n",
    "        \"connor\": \"friend\", \"usnavi\": \"guy\", \"vanessa\": \"girl\"\n",
    "    }\n",
    "\n",
    "    # 2. APPLY NAME REPLACEMENTS (The Fix)\n",
    "    processed_texts = []\n",
    "    for t in texts:\n",
    "        t_lower = t.lower() # Lowercase first\n",
    "        for name, replacement in NAME_REPLACEMENTS.items():\n",
    "            t_lower = t_lower.replace(name, replacement)\n",
    "        processed_texts.append(t_lower)\n",
    "\n",
    "    # 3. VECTORIZE (Use the processed list!)\n",
    "    all_stops = custom_stops + pop_slang + abstract_concepts\n",
    "    \n",
    "    try:\n",
    "        vec = CountVectorizer(stop_words=all_stops).fit(processed_texts)\n",
    "        bag_of_words = vec.transform(processed_texts)\n",
    "        sum_words = bag_of_words.sum(axis=0) \n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "        \n",
    "        # 4. FINAL CLEANUP (Noun Check)\n",
    "        cleaned_list = []\n",
    "        for word, freq in sorted(words_freq, key=lambda x: x[1], reverse=True):\n",
    "            if len(word) <= 2: continue \n",
    "            \n",
    "            # Check Noun (NN, NNS)\n",
    "            # We wrap it in list [] because pos_tag expects a list of tokens\n",
    "            pos_tag = nltk.pos_tag([word])[0][1]\n",
    "            if pos_tag.startswith('NN'):\n",
    "                cleaned_list.append((word, freq))\n",
    "                \n",
    "        return cleaned_list[:n]\n",
    "        \n",
    "    except ValueError:\n",
    "        # Handles empty clusters\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98d4fdd72a410d18",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T17:21:33.140851Z",
     "start_time": "2026-01-11T17:15:20.097697Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-12 02:15:20,096] A new study created in memory with name: no-name-39618001-509f-482e-8413-a21059baa870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Starting Optuna Optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-12 02:15:37,715] Trial 0 finished with value: 0.2716422975063324 and parameters: {'n_clusters': 48}. Best is trial 0 with value: 0.2716422975063324.\n",
      "[I 2026-01-12 02:15:52,311] Trial 1 finished with value: 0.2559882402420044 and parameters: {'n_clusters': 41}. Best is trial 0 with value: 0.2716422975063324.\n",
      "[I 2026-01-12 02:16:12,704] Trial 2 finished with value: 0.27651482820510864 and parameters: {'n_clusters': 59}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:16:31,916] Trial 3 finished with value: 0.27528074383735657 and parameters: {'n_clusters': 60}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:16:51,851] Trial 4 finished with value: 0.27528074383735657 and parameters: {'n_clusters': 60}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:17:09,412] Trial 5 finished with value: 0.266170859336853 and parameters: {'n_clusters': 50}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:17:26,276] Trial 6 finished with value: 0.26966679096221924 and parameters: {'n_clusters': 53}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:17:40,633] Trial 7 finished with value: 0.258674681186676 and parameters: {'n_clusters': 40}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:17:58,040] Trial 8 finished with value: 0.2759794294834137 and parameters: {'n_clusters': 58}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:18:14,909] Trial 9 finished with value: 0.26997604966163635 and parameters: {'n_clusters': 51}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:18:33,948] Trial 10 finished with value: 0.2706632912158966 and parameters: {'n_clusters': 55}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:18:54,381] Trial 11 finished with value: 0.2712375521659851 and parameters: {'n_clusters': 56}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:19:18,189] Trial 12 finished with value: 0.27605685591697693 and parameters: {'n_clusters': 57}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:19:34,513] Trial 13 finished with value: 0.25920554995536804 and parameters: {'n_clusters': 45}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:19:53,193] Trial 14 finished with value: 0.2712375521659851 and parameters: {'n_clusters': 56}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:20:11,924] Trial 15 finished with value: 0.2759794294834137 and parameters: {'n_clusters': 58}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:20:36,230] Trial 16 finished with value: 0.2678925395011902 and parameters: {'n_clusters': 54}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:20:55,547] Trial 17 finished with value: 0.2759794294834137 and parameters: {'n_clusters': 58}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:21:14,681] Trial 18 finished with value: 0.26942986249923706 and parameters: {'n_clusters': 52}. Best is trial 2 with value: 0.27651482820510864.\n",
      "[I 2026-01-12 02:21:33,119] Trial 19 finished with value: 0.2622126638889313 and parameters: {'n_clusters': 47}. Best is trial 2 with value: 0.27651482820510864.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "ðŸ† Best K found: 59\n",
      "ðŸ“ˆ Best Silhouette Score: 0.2765\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 0. ENSURE EMBEDDINGS EXIST\n",
    "if 'embeddings' not in globals():\n",
    "    print(\"â³ Generating Embeddings (this happens once)...\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(corpus)\n",
    "\n",
    "\n",
    "# 1. DEFINE THE OBJECTIVE FUNCTION\n",
    "# Optuna will run this function many times with different 'k' values\n",
    "def objective(trial):\n",
    "    k = trial.suggest_int('n_clusters', 40, 60)\n",
    "\n",
    "    model = KMeans(\n",
    "        n_clusters=k,\n",
    "        random_state=42,\n",
    "        n_init=10\n",
    "    )\n",
    "    labels = model.fit_predict(embeddings)\n",
    "\n",
    "    cohesion = cluster_cohesion(embeddings, labels)\n",
    "\n",
    "    return cohesion  # HIGHER = better semantic cohesion\n",
    "\n",
    "# 2. RUN THE OPTIMIZATION\n",
    "print(\"ðŸ¤– Starting Optuna Optimization...\")\n",
    "study = optuna.create_study(direction='maximize')  # We want MAX separation\n",
    "study.optimize(objective, n_trials=20)  # Try 20 different times\n",
    "\n",
    "# 3. PRINT RESULTS\n",
    "print(\"-\" * 40)\n",
    "print(f\"ðŸ† Best K found: {study.best_params['n_clusters']}\")\n",
    "print(f\"ðŸ“ˆ Best Silhouette Score: {study.best_value:.4f}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 4. OPTIONAL: VISUALIZE\n",
    "optuna.visualization.plot_optimization_history(study).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e39b6d9ca8388e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-12T06:34:13.298605Z",
     "start_time": "2026-01-12T06:33:47.169698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§  Analyzing 12767 lyric segments with SBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 3d3d4059-536c-4490-b8b5-1534b093369d)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "   YOUR VISUAL SHOPPING LIST\n",
      "========================================\n",
      "\n",
      "ðŸ“ CLUSTER 1 (Sample: 1294)\n",
      "   Keywords: ['woah', 'shine', 'shake', 'hear', 'sound', 'dat', 'shot', 'dang', 'fight', 'wolf']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 2 (Sample: 1503)\n",
      "   Keywords: ['mine', 'belong', 'home', 'sorry', 'friend', 'beautiful', 'son', 'show', 'shot', 'years']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 3 (Sample: 1355)\n",
      "   Keywords: ['fight', 'men', 'war', 'captain', 'people', 'deserves', 'lives', 'mercy', 'win', 'home']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 4 (Sample: 1678)\n",
      "   Keywords: ['try', 'game', 'thing', 'fight', 'til', 'something', 'line', 'break', 'monster', 'work']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 5 (Sample: 634)\n",
      "   Keywords: ['soldier', 'president', 'alexander', 'politician', 'washington', 'sir', 'york', 'thomas', 'home', 'congress']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 6 (Sample: 1312)\n",
      "   Keywords: ['room', 'light', 'sky', 'dream', 'happens', 'sun', 'storm', 'home', 'dark', 'place']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 7 (Sample: 1544)\n",
      "   Keywords: ['home', 'house', 'tonight', 'party', 'place', 'round', 'city', 'york', 'work', 'piragua']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 8 (Sample: 1218)\n",
      "   Keywords: ['people', 'friends', 'everyone', 'someone', 'friend', 'school', 'thinks', 'guy', 'guess', 'somebody']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 9 (Sample: 1173)\n",
      "   Keywords: ['queen', 'student', 'beautiful', 'wife', 'eliza', 'sister', 'mom', 'mother', 'friend', 'witch']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 10 (Sample: 1056)\n",
      "   Keywords: ['guy', 'set', 'fire', 'son', 'friend', 'house', 'michael', 'dad', 'father', 'daddy']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nðŸ§  Analyzing {len(corpus)} lyric segments with SBERT...\")\n",
    "\n",
    "NUM_CLUSTERS = 10\n",
    "\n",
    "# A. Embed\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(corpus)\n",
    "\n",
    "# B. Cluster\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42)\n",
    "kmeans.fit(embeddings)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# ==========================================\n",
    "# 4. RESULTS (THE \"SHOPPING LIST\")\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"   YOUR VISUAL SHOPPING LIST\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Helper to find words from the clusters\n",
    "def get_top_keywords(texts, n=8):\n",
    "    vec = CountVectorizer(stop_words='english').fit(texts)\n",
    "    bag_of_words = vec.transform(texts)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    return sorted(words_freq, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "df = pd.DataFrame({'text': corpus, 'cluster': cluster_labels})\n",
    "\n",
    "for i in range(NUM_CLUSTERS):\n",
    "    cluster_text = df[df['cluster'] == i]['text'].tolist()\n",
    "    \n",
    "    # Use the new function\n",
    "    keywords = get_top_keywords_filtered(cluster_text)\n",
    "    \n",
    "    print(f\"\\nðŸ“ CLUSTER {i+1} (Sample: {len(cluster_text)})\")\n",
    "    # Print just the words, not the frequency counts\n",
    "    print(f\"   Keywords: {[k[0] for k in keywords]}\")\n",
    "    print(f\"   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4f5bfedd4f5e412",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-11T09:01:55.441029Z",
     "start_time": "2026-01-11T09:01:55.423297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved as 'kmeans_presets.pkl'\n",
      "âœ… Labels saved as 'preset_labels.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# 1. SAVE THE MODEL\n",
    "# (Make sure 'kmeans' is your trained model variable)\n",
    "# Using joblib is often safer for sklearn models than pickle, but pickle works too.\n",
    "with open('kmeans_presets.pkl', 'wb') as f:\n",
    "    pickle.dump(kmeans, f)\n",
    "print(\"âœ… Model saved as 'kmeans_presets.pkl'\")\n",
    "\n",
    "# 2. SAVE YOUR ASSET MAP\n",
    "# Mapped to match your SBERT Keywords exactly:\n",
    "\n",
    "preset_map = {\n",
    "    0: \"urban_environment\",  # Cluster 1: Money, Piragua, Bag (In The Heights/City)\n",
    "    1: \"urban_environment\",  # Cluster 2: Dance, Shake, Shot (Party/Club Vibe)\n",
    "    2: \"urban_environment\",  # Cluster 3: Heather, Regina, Queen (High School/Social)\n",
    "    3: \"room_environment\",   # Cluster 4: Home, Door, Walk (Domestic/Inside)\n",
    "    4: \"sky_environment\",    # Cluster 5: Sun, Light, Sky, Stars (Lanterns/Magic)\n",
    "    5: \"dark_environment\",   # Cluster 6: Hate, Fight, Care (Conflict/Spooky)\n",
    "    6: \"ocean_environment\",  # Cluster 7: Captain, War, Men, Stand (Epic/Sea)\n",
    "    7: \"fire_environment\",   # Cluster 8: Hamilton, Fire, Burn (Revolution/Rage)\n",
    "    8: \"snow_environment\",   # Cluster 9: Christmas, Merry, Fall (Winter)\n",
    "    9: \"room_environment\"    # Cluster 10: Room, Dream, Sleep (Intimate/Thoughts)\n",
    "}\n",
    "\n",
    "with open('preset_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(preset_map, f)\n",
    "print(\"âœ… Labels saved as 'preset_labels.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Current Corpus Length: 22824\n",
      "ðŸ§  Saved Model Labels:    22824\n",
      "\n",
      "========================================\n",
      "   YOUR VISUAL SHOPPING LIST\n",
      "========================================\n",
      "\n",
      "ðŸ“ CLUSTER 0 (Sample: 2833)\n",
      "   ðŸ·ï¸ Saved Label: urban_environment\n",
      "   Keywords: ['money', 'round', 'something', 'house', 'bag', 'play', 'home', 'piragua', 'thing', 'check']\n",
      "\n",
      "ðŸ“ CLUSTER 1 (Sample: 2779)\n",
      "   ðŸ·ï¸ Saved Label: urban_environment\n",
      "   Keywords: ['woah', 'shake', 'shot', 'hear', 'sound', 'dance', 'dat', 'ride', 'wolf', 'shine']\n",
      "\n",
      "ðŸ“ CLUSTER 2 (Sample: 1805)\n",
      "   ðŸ·ï¸ Saved Label: urban_environment\n",
      "   Keywords: ['queen', 'student', 'side', 'friends', 'taylor', 'friend', 'mother', 'beautiful', 'swift', 'home']\n",
      "\n",
      "ðŸ“ CLUSTER 3 (Sample: 2437)\n",
      "   ðŸ·ï¸ Saved Label: room_environment\n",
      "   Keywords: ['home', 'belong', 'door', 'walk', 'tonight', 'road', 'place', 'try', 'yes', 'house']\n",
      "\n",
      "ðŸ“ CLUSTER 4 (Sample: 1610)\n",
      "   ðŸ·ï¸ Saved Label: sky_environment\n",
      "   Keywords: ['sun', 'light', 'sky', 'lucy', 'storm', 'shine', 'home', 'fire', 'stars', 'dark']\n",
      "\n",
      "ðŸ“ CLUSTER 5 (Sample: 2848)\n",
      "   ðŸ·ï¸ Saved Label: dark_environment\n",
      "   Keywords: ['hate', 'friends', 'people', 'thing', 'care', 'fight', 'talk', 'try', 'someone', 'guess']\n",
      "\n",
      "ðŸ“ CLUSTER 6 (Sample: 2280)\n",
      "   ðŸ·ï¸ Saved Label: ocean_environment\n",
      "   Keywords: ['stand', 'fight', 'men', 'war', 'home', 'peace', 'nation', 'crowd', 'people', 'captain']\n",
      "\n",
      "ðŸ“ CLUSTER 7 (Sample: 1893)\n",
      "   ðŸ·ï¸ Saved Label: fire_environment\n",
      "   Keywords: ['soldier', 'guy', 'alexander', 'president', 'house', 'set', 'politician', 'king', 'friend', 'fire']\n",
      "\n",
      "ðŸ“ CLUSTER 8 (Sample: 1919)\n",
      "   ðŸ·ï¸ Saved Label: snow_environment\n",
      "   Keywords: ['christmas', 'cry', 'beautiful', 'thank', 'merry', 'times', 'sorry', 'fall', 'body', 'lover']\n",
      "\n",
      "ðŸ“ CLUSTER 9 (Sample: 2420)\n",
      "   ðŸ·ï¸ Saved Label: room_environment\n",
      "   Keywords: ['room', 'dream', 'deep', 'thought', 'felt', 'left', 'happens', 'people', 'hell', 'sleep']\n"
     ]
    }
   ],
   "source": [
    "# 1. CORRECT WAY TO LOAD\n",
    "with open('output/pickle/kmeans_presets(musical_and_pop).pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "with open('output/pickle/preset_labels(musical_and_pop).pkl', 'rb') as f:\n",
    "    label_map = pickle.load(f)\n",
    "    \n",
    "# Check the difference between your current text and your saved brain\n",
    "print(f\"ðŸ“‰ Current Corpus Length: {len(corpus)}\")\n",
    "print(f\"ðŸ§  Saved Model Labels:    {len(model.labels_)}\")\n",
    "\n",
    "# 2. RUN YOUR ANALYSIS\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"   YOUR VISUAL SHOPPING LIST\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# âš ï¸ CRITICAL NOTE: 'corpus' must exist in memory!\n",
    "# If this is a new script, you need to re-generate or load the 'corpus' list\n",
    "# exactly as it was during training, otherwise model.labels_ won't match.\n",
    "if 'corpus' not in locals():\n",
    "    print(\"âŒ ERROR: Variable 'corpus' is missing. You need the text data to analyze keywords.\")\n",
    "else:\n",
    "    df = pd.DataFrame({'text': corpus, 'cluster': model.labels_})\n",
    "\n",
    "    # Use NUM_CLUSTERS from the loaded model to be safe\n",
    "    num_clusters = model.n_clusters\n",
    "\n",
    "    for i in range(num_clusters):\n",
    "        cluster_text = df[df['cluster'] == i]['text'].tolist()\n",
    "        \n",
    "        # Use your custom filter function\n",
    "        keywords = get_top_keywords_filtered(cluster_text)\n",
    "        \n",
    "        # Get the label you saved (if available) to verify\n",
    "        saved_label = label_map.get(i, \"Unknown\")\n",
    "        \n",
    "        print(f\"\\nðŸ“ CLUSTER {i} (Sample: {len(cluster_text)})\")\n",
    "        print(f\"   ðŸ·ï¸ Saved Label: {saved_label}\") \n",
    "        print(f\"   Keywords: {[k[0] for k in keywords]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-12T06:22:29.115525Z",
     "start_time": "2026-01-12T06:22:27.664836Z"
    }
   },
   "id": "60001e479b8d0cd4"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "preset_map = {\n",
    "    0: (\"urban_environment\", 1.0),\n",
    "    1: (\"urban_environment\", 0.8),\n",
    "    2: (\"urban_environment\", 0.9),\n",
    "    3: (\"room_environment\", 1.0),\n",
    "    4: (\"sky_environment\", 1.0),\n",
    "    5: (\"dark_environment\", 1.0),\n",
    "    6: (\"ocean_environment\", 1.0),\n",
    "    7: (\"fire_environment\", 1.0),\n",
    "    8: (\"snow_environment\", 1.0),\n",
    "    9: (\"room_environment\", 0.9)\n",
    "}\n",
    "\n",
    "with open('output/pickle/preset_labels(musical_and_pop).pkl', 'wb') as f:\n",
    "    pickle.dump(preset_map, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-12T08:13:31.706406Z",
     "start_time": "2026-01-12T08:13:31.684009Z"
    }
   },
   "id": "af46d1257b94400f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1f0402296000296e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
