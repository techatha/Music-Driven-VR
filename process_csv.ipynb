{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-03T07:34:49.074607Z",
     "start_time": "2026-01-03T07:34:49.070703Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning and Segmenting Lyrics...\n",
      "âœ… Processing complete. Total segments: 6762\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"output/songs_with_lyrics_cleaned.csv\")\n",
    "separated_lyrics = data['lyrics'].dropna()\n",
    "\n",
    "corpus = []\n",
    "\n",
    "print(\"ðŸ§¹ Cleaning and Segmenting Lyrics...\")\n",
    "\n",
    "for lyric_text in separated_lyrics:\n",
    "    if isinstance(lyric_text, str):\n",
    "        text_no_tags = re.sub(r'\\[.*?\\]', ' ', lyric_text, flags=re.DOTALL)\n",
    "        raw_segments = text_no_tags.split('\\n')\n",
    "        clean_segments = [s.strip().lower() for s in raw_segments if len(s.strip()) > 30]\n",
    "        \n",
    "        # Add to your main list\n",
    "        corpus.extend(clean_segments)\n",
    "\n",
    "print(f\"âœ… Processing complete. Total segments: {len(corpus)}\")\n",
    "# Now 'corpus' is ready for SBERT"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-03T07:34:49.613212Z",
     "start_time": "2026-01-03T07:34:49.594183Z"
    }
   },
   "id": "8674b11a076872cc"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/Bay_Techatham/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/Bay_Techatham/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Bay_Techatham/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def get_top_keywords_filtered(texts, n=10):\n",
    "    # 1. EXPANDED STOPWORDS (Grammar + Slang)\n",
    "    custom_stops = list(nltk.corpus.stopwords.words('english'))\n",
    "    pop_slang = [\n",
    "        \"lil\", \"gon\", \"bout\", \"em\", \"ayy\", \"uh\", \"huh\", \"ha\", \"vi\", \"doo\", \"wee\",\n",
    "        \"yeah\", \"oh\", \"baby\", \"know\", \"like\", \"got\", \"just\", \"don\", \"ve\", \"ll\", \n",
    "        \"want\", \"need\", \"love\", \"time\", \"way\", \"make\", \"say\", \"come\", \"go\", \"right\",\n",
    "        \"look\", \"good\", \"feel\", \"really\", \"cause\", \"wanna\", \"gonna\", \"gotta\", \"ain\",\n",
    "        \"girl\", \"boy\", \"man\", \"woman\", \"hey\", \"ooh\", \"whoa\", \"shit\", \"fuck\", \"bitch\",\n",
    "        \"nigga\", \"niggas\", \"damn\", \"ass\", \"tell\", \"think\", \"never\", \"back\", \"let\",\n",
    "        \"swag\", \"yuh\", \"hum\", \"who\", \"what\", \"where\", \"why\", \"top\", \"call\", \"put\",\n",
    "        \"gang\", \"thug\", \"bro\", \"pussy\", \"tryna\", \"chick\", \"girls\", \"slatt\", \"mmh\"\n",
    "    ]\n",
    "    \n",
    "    # 2. ABSTRACT NOUNS BLACKLIST (The \"Non-Visual\" Filter)\n",
    "    # These are nouns/verbs that appear often but you can't build them in 3D.\n",
    "    abstract_concepts = [\n",
    "        \"life\", \"day\", \"night\", \"heart\", \"mind\", \"world\", \"everything\", \"nothing\", \n",
    "        \"things\", \"nothin\", \"songs\", \"song\", \"name\", \"eyes\", \"face\", \"voice\", \n",
    "        \"head\", \"hand\", \"hands\", \"god\", \"soul\", \"mind\", \"pain\", \"hope\", \"wish\", \n",
    "        \"fame\", \"lie\", \"lies\", \"truth\", \"word\", \"words\", \"end\", \"reason\", \"part\",\n",
    "        \"told\", \"saw\", \"knew\", \"met\", \"said\", \"made\", \"found\", \"came\", \"went\", # Past tense verbs\n",
    "        \"die\", \"born\", \"live\", \"dead\", \"death\", \"control\", \"move\", \"wait\", \"hold\",\n",
    "        \"stop\", \"start\", \"change\", \"keep\", \"leave\", \"stay\", \"believe\", \"remember\"\n",
    "    ]\n",
    "\n",
    "    # Combine all lists\n",
    "    all_stops = custom_stops + pop_slang + abstract_concepts\n",
    "\n",
    "    # 3. VECTORIZE\n",
    "    vec = CountVectorizer(stop_words=all_stops).fit(texts)\n",
    "    bag_of_words = vec.transform(texts)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    \n",
    "    # 4. FINAL CLEANUP (Noun Check + Length Check)\n",
    "    cleaned_list = []\n",
    "    for word, freq in sorted(words_freq, key=lambda x: x[1], reverse=True):\n",
    "        if len(word) <= 2: continue # Remove short garbage\n",
    "        \n",
    "        # Verify it's a Noun (NN)\n",
    "        pos_tag = nltk.pos_tag([word])[0][1]\n",
    "        if pos_tag.startswith('NN'):\n",
    "            cleaned_list.append((word, freq))\n",
    "            \n",
    "    return cleaned_list[:n]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-03T07:46:15.458227Z",
     "start_time": "2026-01-03T07:46:15.453261Z"
    }
   },
   "id": "4387bb0afe28e96a"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-03 16:51:22,645] A new study created in memory with name: no-name-53bf9b13-ff4b-45eb-bcd8-650a90a7657e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Starting Optuna Optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-03 16:51:26,826] Trial 0 finished with value: 0.02043554000556469 and parameters: {'n_clusters': 19}. Best is trial 0 with value: 0.02043554000556469.\n",
      "[I 2026-01-03 16:51:29,953] Trial 1 finished with value: 0.01862427219748497 and parameters: {'n_clusters': 17}. Best is trial 0 with value: 0.02043554000556469.\n",
      "[I 2026-01-03 16:51:34,094] Trial 2 finished with value: 0.01531270518898964 and parameters: {'n_clusters': 13}. Best is trial 0 with value: 0.02043554000556469.\n",
      "[I 2026-01-03 16:51:37,912] Trial 3 finished with value: 0.02079150639474392 and parameters: {'n_clusters': 20}. Best is trial 3 with value: 0.02079150639474392.\n",
      "[I 2026-01-03 16:51:42,682] Trial 4 finished with value: 0.019564403221011162 and parameters: {'n_clusters': 30}. Best is trial 3 with value: 0.02079150639474392.\n",
      "[I 2026-01-03 16:51:45,105] Trial 5 finished with value: 0.012676121667027473 and parameters: {'n_clusters': 7}. Best is trial 3 with value: 0.02079150639474392.\n",
      "[I 2026-01-03 16:51:49,010] Trial 6 finished with value: 0.017993029206991196 and parameters: {'n_clusters': 15}. Best is trial 3 with value: 0.02079150639474392.\n",
      "[I 2026-01-03 16:51:53,476] Trial 7 finished with value: 0.02192435972392559 and parameters: {'n_clusters': 22}. Best is trial 7 with value: 0.02192435972392559.\n",
      "[I 2026-01-03 16:51:57,411] Trial 8 finished with value: 0.020717410370707512 and parameters: {'n_clusters': 21}. Best is trial 7 with value: 0.02192435972392559.\n",
      "[I 2026-01-03 16:52:01,228] Trial 9 finished with value: 0.02043554000556469 and parameters: {'n_clusters': 19}. Best is trial 7 with value: 0.02192435972392559.\n",
      "[I 2026-01-03 16:52:06,913] Trial 10 finished with value: 0.023119619116187096 and parameters: {'n_clusters': 28}. Best is trial 10 with value: 0.023119619116187096.\n",
      "[I 2026-01-03 16:52:11,913] Trial 11 finished with value: 0.021037058904767036 and parameters: {'n_clusters': 27}. Best is trial 10 with value: 0.023119619116187096.\n",
      "[I 2026-01-03 16:52:16,539] Trial 12 finished with value: 0.021824464201927185 and parameters: {'n_clusters': 26}. Best is trial 10 with value: 0.023119619116187096.\n",
      "[I 2026-01-03 16:52:21,184] Trial 13 finished with value: 0.021824464201927185 and parameters: {'n_clusters': 26}. Best is trial 10 with value: 0.023119619116187096.\n",
      "[I 2026-01-03 16:52:25,496] Trial 14 finished with value: 0.02129167877137661 and parameters: {'n_clusters': 23}. Best is trial 10 with value: 0.023119619116187096.\n",
      "[I 2026-01-03 16:52:30,173] Trial 15 finished with value: 0.019564403221011162 and parameters: {'n_clusters': 30}. Best is trial 10 with value: 0.023119619116187096.\n",
      "[I 2026-01-03 16:52:34,093] Trial 16 finished with value: 0.0224746186286211 and parameters: {'n_clusters': 24}. Best is trial 10 with value: 0.023119619116187096.\n",
      "[I 2026-01-03 16:52:38,978] Trial 17 finished with value: 0.0222930908203125 and parameters: {'n_clusters': 24}. Best is trial 10 with value: 0.023119619116187096.\n",
      "[I 2026-01-03 16:52:41,830] Trial 18 finished with value: 0.013909848406910896 and parameters: {'n_clusters': 11}. Best is trial 10 with value: 0.023119619116187096.\n",
      "[I 2026-01-03 16:52:46,859] Trial 19 finished with value: 0.023119619116187096 and parameters: {'n_clusters': 28}. Best is trial 10 with value: 0.023119619116187096.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "ðŸ† Best K found: 28\n",
      "ðŸ“ˆ Best Silhouette Score: 0.0231\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# 1. DEFINE THE OBJECTIVE FUNCTION\n",
    "# Optuna will run this function many times with different 'k' values\n",
    "def objective(trial):\n",
    "    # Ask Optuna to try a K between 4 and 15\n",
    "    k = trial.suggest_int('n_clusters', 4, 30)\n",
    "    \n",
    "    # Train the model\n",
    "    model = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    model.fit(embeddings)  # Use your existing SBERT embeddings\n",
    "    \n",
    "    # Calculate score (Higher is better)\n",
    "    # Silhouette score measures how distinct the clusters are\n",
    "    score = silhouette_score(embeddings, model.labels_)\n",
    "    \n",
    "    return score\n",
    "\n",
    "# 2. RUN THE OPTIMIZATION\n",
    "print(\"ðŸ¤– Starting Optuna Optimization...\")\n",
    "study = optuna.create_study(direction='maximize')  # We want MAX separation\n",
    "study.optimize(objective, n_trials=20)  # Try 20 different times\n",
    "\n",
    "# 3. PRINT RESULTS\n",
    "print(\"-\" * 40)\n",
    "print(f\"ðŸ† Best K found: {study.best_params['n_clusters']}\")\n",
    "print(f\"ðŸ“ˆ Best Silhouette Score: {study.best_value:.4f}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 4. OPTIONAL: VISUALIZE\n",
    "# optuna.visualization.plot_optimization_history(study).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-03T07:52:46.867186Z",
     "start_time": "2026-01-03T07:51:22.647768Z"
    }
   },
   "id": "98d4fdd72a410d18"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§  Analyzing 6762 lyric segments with SBERT...\n",
      "\n",
      "========================================\n",
      "   YOUR VISUAL SHOPPING LIST\n",
      "========================================\n",
      "\n",
      "ðŸ“ CLUSTER 1 (Sample: 759)\n",
      "   Keywords: ['talk', 'friends', 'money', 'thought', 'ones', 'kept', 'somebody', 'people', 'care', 'school']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 2 (Sample: 1075)\n",
      "   Keywords: ['money', 'coupe', 'house', 'bag', 'home', 'round', 'light', 'hit', 'body', 'cut']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 3 (Sample: 695)\n",
      "   Keywords: ['lucy', 'heaven', 'shake', 'diamonds', 'party', 'music', 'king', 'sky', 'stole', 'doin']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 4 (Sample: 831)\n",
      "   Keywords: ['hate', 'try', 'learn', 'left', 'break', 'living', 'feels', 'hurt', 'drugs', 'half']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 5 (Sample: 328)\n",
      "   Keywords: ['masa', 'rap', 'gettin', 'hoes', 'pop', 'wayne', 'playin', 'fuckin', 'money', 'chaos']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 6 (Sample: 447)\n",
      "   Keywords: ['ride', 'eat', 'side', 'taylor', 'crib', 'swift', 'till', 'wheels', 'fall', 'bitches']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 7 (Sample: 940)\n",
      "   Keywords: ['yes', 'woah', 'tonight', 'please', 'sickness', 'lover', 'home', 'sad', 'traveller', 'dance']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 8 (Sample: 683)\n",
      "   Keywords: ['sun', 'light', 'fire', 'dark', 'deep', 'cold', 'lights', 'skin', 'something', 'water']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 9 (Sample: 125)\n",
      "   Keywords: ['christmas', 'merry', 'snow', 'everyday', 'angel', 'mine', 'home', 'year', 'side', 'lot']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n",
      "\n",
      "ðŸ“ CLUSTER 10 (Sample: 879)\n",
      "   Keywords: ['violence', 'stand', 'crowd', 'times', 'hurry', 'thought', 'hearts', 'city', 'thing', 'turn']\n",
      "   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nðŸ§  Analyzing {len(corpus)} lyric segments with SBERT...\")\n",
    "\n",
    "NUM_CLUSTERS = 10\n",
    "\n",
    "# A. Embed\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(corpus)\n",
    "\n",
    "# B. Cluster\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42)\n",
    "kmeans.fit(embeddings)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# ==========================================\n",
    "# 4. RESULTS (THE \"SHOPPING LIST\")\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"   YOUR VISUAL SHOPPING LIST\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Helper to find words from the clusters\n",
    "def get_top_keywords(texts, n=8):\n",
    "    vec = CountVectorizer(stop_words='english').fit(texts)\n",
    "    bag_of_words = vec.transform(texts)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    return sorted(words_freq, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "df = pd.DataFrame({'text': corpus, 'cluster': cluster_labels})\n",
    "\n",
    "for i in range(NUM_CLUSTERS):\n",
    "    cluster_text = df[df['cluster'] == i]['text'].tolist()\n",
    "    \n",
    "    # Use the new function\n",
    "    keywords = get_top_keywords_filtered(cluster_text)\n",
    "    \n",
    "    print(f\"\\nðŸ“ CLUSTER {i+1} (Sample: {len(cluster_text)})\")\n",
    "    # Print just the words, not the frequency counts\n",
    "    print(f\"   Keywords: {[k[0] for k in keywords]}\")\n",
    "    print(f\"   ðŸ‘‰ DECISION: What 3D asset matches these nouns?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-03T08:00:00.160872Z",
     "start_time": "2026-01-03T07:59:51.019643Z"
    }
   },
   "id": "e39b6d9ca8388e"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved as 'kmeans_presets.pkl'\n",
      "âœ… Labels saved as 'preset_labels.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('kmeans_presets.pkl', 'wb') as f:\n",
    "    pickle.dump(kmeans, f)\n",
    "print(\"âœ… Model saved as 'kmeans_presets.pkl'\")\n",
    "\n",
    "# 2. SAVE YOUR ASSET MAP\n",
    "# Update this dict based on your final 4-8 decisions\n",
    "preset_map = {\n",
    "    0: \"room_environment\",   # Cluster 1 (Friends/School)\n",
    "    1: \"urban_environment\",  # Cluster 2 (Money/Coupe)\n",
    "    2: \"sky_environment\",    # Cluster 3 (Heaven/Sky)\n",
    "    3: \"dark_environment\",   # Cluster 4 (Hate/Drugs)\n",
    "    4: \"urban_environment\",  # Cluster 5 (Rap/Chaos)\n",
    "    5: \"urban_environment\",  # Cluster 6 (Ride/Crib)\n",
    "    6: \"room_environment\",   # Cluster 7 (Lover/Tonight)\n",
    "    7: \"fire_environment\",   # Cluster 8 (Sun/Fire)\n",
    "    8: \"snow_environment\",   # Cluster 9 (Christmas/Snow)\n",
    "    9: \"urban_environment\"   # Cluster 10 (Violence/City)\n",
    "}\n",
    "\n",
    "with open('preset_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(preset_map, f)\n",
    "print(\"âœ… Labels saved as 'preset_labels.pkl'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-03T08:02:32.553111Z",
     "start_time": "2026-01-03T08:02:32.540405Z"
    }
   },
   "id": "a4f5bfedd4f5e412"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "924d9ae853c7be4e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
